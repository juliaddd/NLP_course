{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiewiqJVPI6D"
   },
   "source": [
    "# **Practice 1. Introduction to Text Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NrEE6fcgEpP"
   },
   "source": [
    "## 1. Introduction to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isOR1uZ1gTCX"
   },
   "source": [
    "Python is programing language that provides high levels of freedom to code following its syntactics rules. However, there are some recommendations to follow some good practices programming with Python. We recommend to consult them before starting to program with Python.\n",
    "\n",
    "* [Python guidelines: PEP 8](https://www.python.org/dev/peps/pep-0008/)\n",
    "\n",
    "Other Python guidelines:\n",
    "\n",
    "* [Python guidelines by Google](https://google.github.io/styleguide/pyguide.html).\n",
    "\n",
    "Next, some basic programing concepts to start coding with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg-FkToOhuGQ"
   },
   "source": [
    "### Lists and tuples\n",
    "\n",
    "Another very important data type that we are going to use are sequences: tuples and lists. Both are ordered sets of elements: tuples are delimited by parentheses ( ) and lists by square brackets [ ].\n",
    "\n",
    "Differences:\n",
    "* A list can be altered, a tuple cannot. Tuples are \"immutable\".\n",
    "* A tuple can be used as a key in a dictionary, a list cannot.\n",
    "* A tuple consumes less memory than a list.\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1mUacrOjBGH"
   },
   "outputs": [],
   "source": [
    "mi_lista = [1, 2, \"5\", 2]\n",
    "mi_tupla = (1, 2, \"5\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBnjWVrLkb7W"
   },
   "outputs": [],
   "source": [
    "# Podemos comprobar si un elemento está o no dentro de una secuencia\n",
    "print(2 in mi_lista)\n",
    "print(2 not in mi_tupla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwLGf7-jkaJf"
   },
   "outputs": [],
   "source": [
    "# Usamos len() para extraer la cantidad de elementos de la secuencia.\n",
    "print(len(mi_lista))\n",
    "print(len(mi_tupla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYYb1Vp5kWt_"
   },
   "outputs": [],
   "source": [
    "mi_lista.append(\"A\") # Añade el caracter A al final de la lista\n",
    "mi_lista.extend([\"B\", \"C\"]) # Añade los caracteres B y C al final\n",
    "mi_lista.insert(0, \"D\") # Añade el caracter D en la posición 0 (al principio)\n",
    "mi_lista.remove(2) # Elimina la primera ocurrencia del elemento 2\n",
    "dato = mi_lista.pop(0) # Extrae el primer elemento y lo devuelve\n",
    "dato = mi_lista.pop() # Por defecto, extrae el último elemento. Igual que mi_lista.pop(-1)\n",
    "\n",
    "print(mi_lista)\n",
    "print(dato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuMdErb8kVFA"
   },
   "outputs": [],
   "source": [
    "# Concatenar listas y tuplas\n",
    "lista_1 = [1, 2, 3]\n",
    "lista_2 = [4, 5, 6]\n",
    "lista3 = lista_1 + lista_2\n",
    "print(lista3)\n",
    "\n",
    "tupla_1 = (1, 2, 3)\n",
    "tupla_2 = (4, 5, 6)\n",
    "tupla_3 = tupla_1 + tupla_2\n",
    "print(tupla_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFtwVla6kjrf"
   },
   "outputs": [],
   "source": [
    "# Para buscar y ordenar también tenemos varios métodos\n",
    "estudiantes = ['Rosa', 'Antonio', 'Ismael', 'Anabel', 'Miguel', 'Cristina', 'Lucas', 'Miguel']\n",
    "\n",
    "estudiantes.reverse()   # Invierte el orden de los elementos\n",
    "print(\".reverse()\", estudiantes)\n",
    "\n",
    "estudiantes.sort()      # Ordena los elementos (alfabéticamente para str)\n",
    "print(\".sort()\", estudiantes)\n",
    "\n",
    "estudiantes.sort(reverse=True)  # Ordena los elementos en orden inverso\n",
    "print(\".sort(reverse=True)\", estudiantes)\n",
    "\n",
    "print(f\"Miguel aparece {estudiantes.count('Miguel')} veces.\")   # Cuenta el número de apariciones del elemento buscado\n",
    "print(f\"Miguel aparece en la posición {estudiantes.index('Miguel')}\")   # Extrae la posición del elemento buscado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz37vkmwnK7P"
   },
   "source": [
    "### Rangos\n",
    "\n",
    "Los rangos son tipos especiales en Python que devuelven un objeto que produce una secuencia de enteros desde `start` (incluido) hasta `stop` (no incluido) saltando `step` (opcional). Si solo se especifica un valor, Python lo interpretará como el valor de `stop`, y `start` valdrá 0.\n",
    "\n",
    "Son especialmente útiles para iterar por ellos dentro de un bucle for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqfKXYJapSrg"
   },
   "outputs": [],
   "source": [
    "print(list(range(6)))\n",
    "print(list(range(0, 6, 2)))\n",
    "print(list(range(5, -1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQikxXZxn6c3"
   },
   "source": [
    "### Dictionaries\n",
    "\n",
    "In Python, a dictionary is an unordered collection of values that are accessed through a key. This means that instead of accessing information using a numerical index (position), as is the case with lists and tuples, it is possible to access **values** through their **keys**, which can be of various types.\n",
    "\n",
    "Keys are **unique** within a dictionary, meaning that there cannot be a dictionary that has the same key twice. If a value is assigned to an existing key, the previous value is replaced.\n",
    "\n",
    "There is no direct way to access a key through its value, and nothing prevents the same value from being assigned to different keys.\n",
    "\n",
    "The information stored in dictionaries does not have a particular order. Neither by key, nor by value, nor by the order in which they have been added to the dictionary.\n",
    "\n",
    "Any **immutable variable** can be a **key** in a dictionary: strings, integers, tuples (with immutable values in their members), etc. **There are no restrictions on the values** that the dictionary can contain; any type can be the value: lists, strings, tuples, other dictionaries, objects...\n",
    "\n",
    "Similar to lists, it is possible to define a dictionary directly with the members it will contain, or to initialize an empty dictionary and then add values one by one or in bulk.\n",
    "\n",
    "To define it along with the members it will contain, the list of values is enclosed in curly braces, the key-value pairs are separated by commas, and the key and value are separated by a colon \":\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJf5KytLph9_"
   },
   "outputs": [],
   "source": [
    "punto = {\"x\": 2, \"y\": 1, \"z\": 4}\n",
    "\n",
    "materias = {}\n",
    "materias[\"lunes\"] = [6103, 7540]\n",
    "materias[\"martes\"] = [6201]\n",
    "materias[\"miercoles\"] = [6103, 7540]\n",
    "materias[\"jueves\"] = []\n",
    "materias[\"viernes\"] = [6201]\n",
    "\n",
    "# Para acceder al valor asociado a una determinada clave, se hace de la misma\n",
    "# forma que con las listas, pero utilizando la clave elegida en lugar del índice.\n",
    "\n",
    "valor = materias[\"lunes\"]\n",
    "print(valor)\n",
    "\n",
    "# También se puede acceder a los valores de un diccionario con el método\n",
    "# \"get(key, value)\". Si la clave no existe, devuelve value.\n",
    "\n",
    "valor = materias.get(\"sábado\", [777])\n",
    "print(valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0sx-8kyqAse"
   },
   "source": [
    "### Methods\n",
    "\n",
    "In Python, the definition of functions is done using the `def` instruction followed by a descriptive function `name`, for which the same rules as for variable names apply, followed by opening and closing parentheses. The definition of the function header ends with a colon (:). The algorithm that makes up the function will be indented with 4 spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHgC0db6qE9m"
   },
   "outputs": [],
   "source": [
    "def my_method():\n",
    "    print('Hello world!')\n",
    "\n",
    "my_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOI4j5sOqGAW"
   },
   "source": [
    "When defining the function, you can specify as many arguments or input parameters as needed, which may or may not have default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKSi5pFNr-88"
   },
   "source": [
    "### Files\n",
    "\n",
    "Python has several modes for reading and writing files, which are specified as a parameter to the open() function:\n",
    "\n",
    "* \"r\": Read - Default value. Opens an existing file for reading; returns an error if the file does not exist.\n",
    "* \"a\": Append - Opens an existing file for appending content; creates the file if it does not exist.\n",
    "* \"w\": Write - Opens a file for writing content; creates the file if it does not exist.\n",
    "* \"x\": Create - Creates a new file; returns an error if the file already exists.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJxCwAzTtTaH"
   },
   "source": [
    "### Libraries\n",
    "\n",
    "A library is a collection of modules that contain code that can be reused in different programs. Python has a wide variety of libraries natively, but it is possible to install many more using the bash command `pip install <library>` (in Colab, `!` is used to execute bash commands).\n",
    "\n",
    "There are several ways to import a library:\n",
    "\n",
    "* `import <library>`\n",
    "* `import <library> as <alias>`\n",
    "* `from <library> import <module>`\n",
    "\n",
    "Some of the most commonly used built-in libraries are:\n",
    "\n",
    "* `os` - Operating system dependent functionalities.\n",
    "* `math` - Mathematical functions.\n",
    "* `random` - Generation of pseudo-random numbers.\n",
    "* `datetime` - Date-related functions.\n",
    "\n",
    "Some of the most commonly used installable libraries are (being so popular, Colab has them pre-installed, but to use them locally on your computer you would have to install them):\n",
    "\n",
    "* `numpy` - For using numerical arrays. Typically imported under the alias np (import numpy as np).\n",
    "* `pandas` - For analysis of datasets in csv, tsv, xlsl, etc. files. Typically imported under the alias pd (import pandas as pd).\n",
    "* `sklearn` - For machine learning.\"\n",
    "* `polar` - data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vextyh1qtXBH"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(f\"Fecha actual: {datetime.now()}\") # Creation of data\n",
    "\n",
    "import math\n",
    "print(math.sqrt(144))   # Square root\n",
    "\n",
    "import numpy as np\n",
    "array_aleatorio = np.random.rand(5) # Array of size 5 with random values.\n",
    "print(array_aleatorio)\n",
    "print(array_aleatorio.argmax()) # The index with max value in a array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F5W3v6cgJLH"
   },
   "source": [
    "## 2. Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMTAs5WJQE5h"
   },
   "source": [
    "### ¿What is **NLTK**?\n",
    "\n",
    "[NLTK](http://www.nltk.org/) is a library that provides interfaces for easily using a large number of lexical resources, as well as methods for text processing, analysis, and classification.\n",
    "\n",
    "The library has an associated book, which, in addition to instructing on its use, explains many concepts of NLP: http://www.nltk.org/book/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HFMyVWJQosI"
   },
   "source": [
    "#### 1. Instalando NLTK en notebook\n",
    "\n",
    "Este notebook tiene algunas dependencias, la mayoría de las cuales se pueden instalar a través del gestor de paquetes de python `pip`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucZaAklAQ369"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fuc95vvxPdOv"
   },
   "source": [
    "### Import and Donwloading of linguistic resourcesImportación y Descarga de Recursos\n",
    "\n",
    "Using NLTK requires importing it. NLTK is more than just a library, as it offers the download of linguistic resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wedh_QS1P5-t"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqTAibyxQS_e"
   },
   "source": [
    "### Text preprocessing\n",
    "\n",
    "The preprocessing is linked to *tokenization* and sentence splitting. There are some specific libraries or packages for that operations, as the package *punkt*.\n",
    "\n",
    "To install it: `nltk.download('punkt')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RjzqTRAWQN5z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739802621871,
     "user_tz": -60,
     "elapsed": 6809,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "e94eed9e-8e71-4825-9714-c5cbb8a257f3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g11TpIIjQbsr"
   },
   "source": [
    "#### 1. Sentence splitting\n",
    "\n",
    "The standard method for sentence splitting is:\n",
    "\n",
    "```\n",
    "sent_tokenize(text, language='english')\n",
    "```\n",
    "\n",
    "This function splits the text passed as an argument into sentences using the language we want to analyze. This function uses a language model including characters that mark the beginning and end of a sentence, and they are available for 17 European languages (Spanish, English, Dutch, French...). By default, if no language is specified, the English model is used.\n",
    "\n",
    "Let's see an example. First, we import the sent_tokenize function and then call it, passing the text we want to split as an argument. The data type it returns is a list containing the sentences of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pti_1PXvRPKK"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7fyrXL5S0ED"
   },
   "outputs": [],
   "source": [
    "text = \"Esto es una oración de prueba. ¿También divide las preguntas, Sr. Smith? Además, este tokenizador no separa por comas.\"\n",
    "sent_tokenize(text, language=\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnb-iPMZVdo9"
   },
   "source": [
    "#### 2. División de las oraciones en palabras (*tokenization*)\n",
    "\n",
    "Una vez separado el texto en oraciones vamos a ver cómo dividir una oración en palabras, concretamente en tokens. La forma básica de *tokenización* consiste en separar el texto en tokens por medio de espacios y signos de puntuación. Para ello, nosotros vamos a utilizar el tokenizador *TreebankWordTokenizer* (aunque hay muchos más).\n",
    "\n",
    "Lo primero que debemos hacer será importar el tokenizador y posteriomente instanciar la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kjwtrrTVuFR"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT0zvqq6VvWf"
   },
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fi2BubCV9c1"
   },
   "outputs": [],
   "source": [
    "text = \"Esto es una oración de prueba. ¿También divide las preguntas, Sr. Smith? Además, este tokenizador no separa por comas.\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXbibxKoW-1e"
   },
   "source": [
    "NLTK provides other tokenizers such as `RegexpTokenizer`, `WhitespaceTokenizer`, `SpaceTokenizer`, `WordPunctTokenizer`, etc., which you should try out to complete the exercises in this practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZGqPHHMXStT"
   },
   "source": [
    "#### 3. Stopwords removal\n",
    "\n",
    "Stop words are words that lack meaning on their own. They are usually articles, pronouns, prepositions...\n",
    "\n",
    "In some Natural Language Processing tasks, it is useful to remove these words, so next we are going to see how we could eliminate the stop words that are part of a set of tokens.\n",
    "\n",
    "NLTK has a list of stop words for different languages. Let's see how it is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmsyNDFlWBvN"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fDte3XWb96H"
   },
   "outputs": [],
   "source": [
    "spanish_stops = stopwords.words('spanish')\n",
    "print(spanish_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60nalenXcTJF"
   },
   "source": [
    "Next, given a list of words or tokens, we are going to filter them to remove those words that are considered stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gsKQYHRcCpO"
   },
   "outputs": [],
   "source": [
    "words = ['Esto', 'es', 'una', 'oración', 'de', 'prueba.', '¿También', 'divide', 'las', 'preguntas', ',', 'Sr.', 'Smith', '?', 'Además', ',', 'este', 'tokenizador', 'no', 'separa', 'por', 'comas', '.']\n",
    "\n",
    "\"\"\"\n",
    "filtered = []\n",
    "for word in words:\n",
    "#   if word not in spanish_stops:\n",
    "#     filtered.append(word)\n",
    "\"\"\"\n",
    "filtered = [word for word in words if word not in spanish_stops]\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_D_WJHvcyM2"
   },
   "source": [
    "#### 4. Stemming\n",
    "\n",
    "`Stemming` is the technique used to remove the affixes of a word with the objective of obtaining its root or stem. For example, the stem of 'biblioteca' is 'bibliotec'.\n",
    "\n",
    "This method is often used in information retrieval systems for word indexing because, instead of storing all forms of a word, it allows storing only the stems, reducing the index size and improving the results.\n",
    "\n",
    "There are different stemming algorithms: Porter Stemmer, Lancaster Stemmer, Snowball Stemmer...\n",
    "\n",
    "NLTK has an implementation of some of these algorithms that are very easy to use. Simply instantiate the class, for example, PorterStemmer, and call the stem() method with the word for which you want to obtain its stem.\n",
    "\n",
    "Next, we are going to see an example of how to obtain the stems of a list of tokens using the Snowball algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyVl700XdZcH"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tKO6C3Rd7d_"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsvwPucfd8ko"
   },
   "outputs": [],
   "source": [
    "print(stemmer.stem(\"corriendo\"))\n",
    "print(stemmer.stem(\"biblioteca\"))\n",
    "print(stemmer.stem(\"aburridos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgoOa8sOAb6E"
   },
   "source": [
    "#### 5. BPE: The tokenizer of some LLMs (ChatGPT among others)\n",
    "\n",
    "Byte-Pair Encoding (BPE) was initially developed as an algorithm for text compression, and OpenAI used it for tokenization during the pre-training of the GPT model, although today it is used by many other Transformer models such as the GPT family, RoBERTa, Llama-3, or Gemma.\n",
    "\n",
    "BPE iteratively replaces the most frequent pair of elements with a new element that was not contained in the initial dataset until the desired vocabulary size is reached. For example:\n",
    "\n",
    "Starting with a corpus with the following 5 words:\n",
    "\n",
    "```\n",
    "\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
    "```\n",
    "\n",
    "The vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. In each step of the training of this tokenizer, the algorithm will search for the most frequent pair of consecutive tokens and merge them. Thus, the first rule learned by this tokenizer would be: (\"u\", \"g\") -> \"ug\" because this pair appears 3 times in the corpus, resulting in the following updated vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]. This process is repeated as many times as necessary until the desired vocabulary size is reached (currently 8). The vocabulary of modern tokenizers usually is around 100k tokens.\n",
    "\n",
    "Pre-trained tokenizers from OpenAI can be used through the Tiktoken library\" (https://github.com/openai/tiktoken).\n",
    "\n",
    "```\n",
    "!pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "encoded_text = encoding.encode(\"tiktoken is great!\")\n",
    "print(encoded_text)\n",
    "\n",
    "print(encoding.decode(encoded_text))\n",
    "print([encoding.decode_single_token_bytes(token) for token in encoded_text])\n",
    "\n",
    "```\n",
    "\n",
    "* More info at: https://huggingface.co/learn/nlp-course/en/chapter6/5\n",
    "\n",
    "* Testing tools: https://tiktokenizer.vercel.app/?model=gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTr79Th_ImAV"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "encoded_text = encoding.encode(\"Fragmentación de oraciones mediante el tokenizador de GPT4o.\")\n",
    "\n",
    "print(\"Texto codificado:\", encoded_text)\n",
    "print(\"Texto decodificado:\", encoding.decode(encoded_text))\n",
    "print(\"Visualización de tokens independientes:\", [encoding.decode_single_token_bytes(token) for token in encoded_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fClmznfegbY"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jmVlWt9xTju"
   },
   "source": [
    "The results of this first practice should be submitted to PLATEA by **11:59 PM on February 17, 2025**. You should submit this same notebook with the .ipynb extension, renaming it as follows: pr1_user1_user2.ipynb. Replace \"user1\" and \"user2\" with your email aliases.\n",
    "\n",
    "For the development of these exercises, you must use the collection of documents from PLATEA in the \"Material Complementario\" folder called \"colección_SciELO_PLN\".\n",
    "\n",
    "This collection is composed of 25 files in XML format. You should process each file and consider the text included in the <dc:description xml:lang=\"en\"> tag."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwbcnVfUWrAk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739803288688,
     "user_tz": -60,
     "elapsed": 21844,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "a0b00f29-5780-4e46-ec13-95e03ccdb972"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_path = \"/content/drive/MyDrive/NLP/colección_SciELO_PLN\"\n",
    "# Create an empty dictionary to store descriptions for each file\n",
    "file_data = []\n",
    "# Iterate through each file in the data directory\n",
    "for filename in os.listdir(data_path):\n",
    "  if filename.endswith(\".xml\"):\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot() # Get the root element of the XML tree\n",
    "    file_descriptions = []\n",
    "    description_element = root.find(\".//{http://purl.org/dc/elements/1.1/}description[@{http://www.w3.org/XML/1998/namespace}lang='en']\")\n",
    "    if description_element is not None:\n",
    "      description = description_element.text\n",
    "      file_data.append({\n",
    "          \"filename\": filename,\n",
    "          \"description\": description\n",
    "      })\n",
    "\n",
    "\n",
    "for item in file_data:\n",
    "    print(f\"File: {item['filename']}\")\n",
    "    print(f\"Description: {item['description']}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WswpXtdHWuAA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739805245802,
     "user_tz": -60,
     "elapsed": 294,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "dcf18d98-e387-411f-f67d-fabbb4e31272"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File: S0211-69952009000500006.xml\n",
      "Description: Hemodialysis (HD) patients have an impaired response to hepatitis B (HB) vaccines, and the persistence of immunity, the efficacy of revaccination and the periodicity of postvaccination testing are not well defined. We present the experience during 18 years in an outpatient dialysis center of 136 HD patients who completed a HB vaccination program consisting in 3 doses of 40 µg intramuscular recombinant B vaccine (Engerix-B). In all patients anti-HBs titers were determined annually and in 31 patients every 6 months. Nonresponders patients and responders patients that lost their antibodies (<10 UI/ml) received annually a booster double dose of vaccine. Seventy-four patients (54.4%) developed immunity and the remaining 62 patients were considered nonresponders. When compared both groups, gender and the etiology of chronic kidney disease did not differ between the two groups; nevertheless, nonresponders patients were significantly older than responders. After 1 year of followup, 32% of responders had no detectable anti-HBs levels, and only 18% of patients remained immunoreactive 6 years afer vaccination. The peak anti-HBs titer immediately after completion of the vaccination schedule was found to be a major predictor of maintaining immunity: 75% of patients with anti-HBs titers greater than 1000 IU/ml remained immunoreactive 3 years after vaccination compared to 47% of patients with titers between 100-999 IU/ml (p = 0.08) and 34% of patients with titers between 11-99 IU/ml (p = 0.02). The administration of additional doses of vaccine were effective in 24% of the nonresponders patients, and 69% of them remained seropositive at the end of the 1-year follow up. Repeated booster doses of vaccine in nonresponders patients to the first booster dose afforded seroconversion in 19.6% of the patients. Performing post-vaccination testing every six months it would have allowed to give booster doses of vaccine in 16% of responder patients before the annual period. Conclusion: This current study demonstrates that a HB vaccination schedule with a regular serological followup and repeated booster doses , affords an acceptable seroprotection in HD patients.\n",
      "File: S0211-69952009000600013.xml\n",
      "Description: Autosomal dominant polycystic kidney disease is a multiorganic hereditary disorder. It is responsible for 7-10% of cases of end stage renal failure. It is caused by mutations in the genes PKD1 and PKD2. Both polycystic kidney disease's forms have a pathogeny and similar clinic, but in the patients with mutation in PKD2, the clinical manifestations appear later and the progression to end stage renal failure happens 10 years later than in the patients with mutation in PKD1. The diagnosis of this disease can be performed through ultrasounds, but the molecular diagnosis offers some advantages, such as the early detection of asymptomatic individuals who carry this genetic defect, in order to perform a preventive monitoring and genetic counselling. In this work, we present the results of the mutational analysis of the PKD2 gene in 18 patients diagnosed with autosomal dominant polycystic kidney disease. The objectives of this work were to analyze the profitability of the genetic study compared with the radiologic study, and perform an early genetic diagnosis in the descendants of patients who were affected by a mutation in the PKD2 gene, trying to establish a correlation between phenotype and genotype. After the genetic analysis, only one family was diagnosed with a mutation in exon 13 of the PKD2 gene (5.56%), which consists on a substitution of the nucleotide adenosine by cytosine (c.2398A>C), which implies that the amino acid methionine is replaced by leucine (p.800Met>Leu). In our population, contrary to what was published in the literature, the mutation of the gene was clinically significant and did segregate with the disease. All the members with a clinical and ultrasound diagnosis of polycystic renal disease presented the abovementioned mutation. We could not confirm any clinicalgenetic correlation. Due to the high prevalence of chronic renal failure and terminal chronic renal failure secondary to polycystic kidney disease in our study, an early genetic diagnosis would involve a better prognosis in connection with a closer clinical monitoring.\n",
      "File: S0211-69952009000500012.xml\n",
      "Description: Introduction: Patients with Chronic Kidney Disease (CRD) often have cardiovascular disease that is the main cause of morbidity and mortality. Oxidative stress and a subclinical inflammation are crucial factors in its development. The aim of this study was to assess the oxidation of the main molecular groups in patients with advanced renal disease without dialysis and to determinate the best biomarker to assess this stress. Patients and Methods: We performed an observational study to measure the most important oxidative biomarkers in 32 patients with stage 4 CKD (MDRD = 22.1 ± 1.08ml/min) compared with the values obtained in a control group. In the peripheral lymphocytes we measured, the lipid peroxidation by Malondialdehide (MDA) and F2 Isoprostanes in plasma; protein oxidation by glutathione oxidized/reduced ratio (GSSG/GSH) in peripheral lymphocytes and protein carbonyls in plasma and the oxidative damage in genetic material by modified nucleotide base 8-deoxiguanosina oxo -(8-oxodG), after isolating nuclear and mitochondrial DNA. We also studied the antioxidant defences with superoxide dismutase (SOD), glutathione peroxidase (GPx), glutathione reductase (GSR) and catalase (CAT) in peripheral lymphocytes. We studied the correlation between oxidative stress and the renal function and oxidative stress and co-morbidity factors. Results: All biomarkers showed important differences in comparison with the control subjects. F2 Isoprostanes: 821.89 ± 300.47ng/ml vs. 270 (95.66) * ng/ml (p < 0.000), MDA 0.11 (0.11) * vs. 0.7 ± 0.31nmol/mg prot (p < 0.000). GSSG/GSH: 6.89 ± 1.91 vs. 1.39 ± 0.75 (p < 0.000), protein carbonyls: 7.41 ± 0.84 vs. 3.63 (1.12) *. Nuclear 8-oxo-dG 7.88 (2.32) vs. 2.96 (1.78) * mitochondrial 8-oxo-dG: 15.73 ± 2.28 vs. 13.85 ± 1.44 (p < 0.05). The Antioxidant enzymes also showed differences. Nuclear 8-oxo-dG demonstrated an important relationship with the rest of the biomarkers, homocystein (r = 0.305, p < 0.05), lipoprotein (a) (r = 0.375, p < 0.01), mitochondrial 8-oxodG (r = 0.411, p < 0.05), GSSH/GSH (r= 0.595, p < 0.001) and protein carbonyls (r = 0.489, p < 0.05). There was an inverse correlation with total protein (r = -0.247, p < 0.01), GSH (r = -0.648, p < 0.000), GSR (r = -0.563, p < 0.001) and SOD (r = -0.497, p < 0.000). We did not find any correlation between these parameters and renal function. The presence of diabetes or the treatment with statins did not show significant differences. * Median (Interquartile range). Conclusion: There is an important oxidative stress in patients with advanced renal disease, probably established during the early stages of disease. Of the studied parameters, the nuclear 8-oxo-dG is the best marker for oxidative stress in CRD.\n",
      "File: S0211-69952009000500008.xml\n",
      "Description: Introduction: To guarantee continuity and equity in the clinical assistance of patients on hemodialysis in extrahospitalary centers (EC) a close relationship and a good level of communication between them and their reference hospitals (RH) is essential. The aim of this study was to assess the present situation of this relationship in our country (Spain) so as to be able to detect improvement opportunities. Methods: Descriptive and transversal study using two self-report anonymous surveys: one for EC (81 questions) and one for RH (56 questions) sent by e-mail to all Spanish EC and RH registered in the Spanish Society of Nephrology. Results: We received answers from 80 EC and 30 RH. 70% of the EC were managed by multinational companies; only 16% EC were placed in a hospital. 64% of the EC need to employ nonnephrological medical staff. Nearly 40% of the EC nephrologists also go on duty at their RH. More than three quarters of the EC nephrologists are alone during their workday. Bidirectional telephone communication is very frequent between EC and RH. Around a third of the patients sent from RH to EC arrive without current viral serology and/or without a functioning vascular access. Most of the patients sent from EC to RH bring an up-to-date complete medical report. 41,3% of the EC answered that they were usually consulted by their RH doctor colleagues about decisions to be taken regarding their patients. Routine blood and other medical protocol tests of CE are well defined in the formal agreement with their RH in 65% of the cases, although they can be modified by the EC through consensus with the RH in more than 50% of the cases. 60% of the EC can directly consult other specialists in the RH but more than 50% need to do so through the RH nephrologist. Parenteral medication used in the ECs is mostly supplied by their RH, but a third of ECs have some limitations with uncommon or not concert-specified parenteral drugs. RHs refer that most of the vascular accesses are done in the hospital, whereas ECs say that this is true only in half of the cases. More than a third of the fistulae of predialysis patients are done in the ECs as part of their collaboration with RHs. The majority of ECs can share the decision about patients' inclusion in renal transplant waiting list. In only a fifth of the cases is there a common database between CE and RH, and less than half share common protocols or objectives. 62,5% of CEs participate with RHs in clinical trials. More than half of the dialysis private companies provide continuous training and education to their ECs personnel, either directly through the company or facilitating assistance to courses or congresses. Conclusions: Some of the relationship aspects that appear to be clearly improvable are: CEs nephrologist solitude and their limited access to continuous training and education, an adequate referral of the patients from the RHs, CEs nephrologist's autonomy at making consultations to specialists or their limitations when asking for hospital medications. A closer relationship between CEs and RHs is of the utmost importance in guaranteeing continuity and equity in the clinical assistance of our hemodialysis patients. The creation of a debate forum would favour discussion and common resolution of such aspects.\n",
      "File: S0211-69952009000500014.xml\n",
      "Description: Celiac disease results from the interaction between gluten and immune, genetic, and environmental factors. Although the main clinical manifestations are derived from gastrointestinal system, it has been described some renal diseases, especially chronic glomerulonephritis. We describe a young female patient with celiac disease which appears after delivery. Moreover, she develops simultaneously nephrotic proteinuria and microhematuria as a result of membranous nephropathy. The treatment with gluten-free diet and other conservative measures (ACEI and statin) is followed by clinical improvement and simultaneous decrease of tissue antitransglutaminase IgA-antibodies and complete remission of proteinuria. We review the relationship between celiac disease and membranous nephropathy and the role of diet in the management of both diseases.\n",
      "File: S0211-69952009000600009.xml\n",
      "Description: Background: Fungal peritonitis is a rare but serious complication in patients undergoing continuous ambulatory peritoneal dialysis (CAPD). Methods: During a ten-year period (1999-2008), from a total of 175 patients with chronic renal failure undergoing CAPD, we retrospectively studied 10 cases of fungal peritonitis analyzing the predisposing factors, clinical aspects, etiological agents and treatment. Diagnosis was based on elevated CAPD effluent count (>100/µl) and isolation of fungi on culture. Results: Fungal peritonitis represented 3.6% of all peritonitis episodes. Nine patients had a history of previous bacterial peritonitis and all of them were under antibiotic therapy. Other common findings were: age higher than 70 years old (50%) and diabetes mellitus (40%). Direct microscopic examination of the peritoneal fluid was useful for the suspicion of fungal infection in six patients (60%). The responsible agents for peritonitis were: Candida parapsilosis (4), C. albicans (2), C. tropicalis (1), C. glabrata (1), C. famata (1) and Fusarium oxysporum (1). Intraperitoneal and oral fluconazole, intravenous and oral voriconazole and intravenous amphotericin B were the antifungal agents used in the treatment. As a result of fungal infection, eight patients were transferred to hemodialysis. One patient died before the diagnosis and three other during the episode of peritonitis. Conclusions: Patients with previous bacterial peritonitis and antibiotic treatment were at greater risk of developing fungal peritonitis. C. parapsilosis was the most common pathogen. For the successful management of fungal peritonitis besides the antifungal therapy, peritoneal catheter removal was necessary in 60% of patients.\n",
      "File: S0211-69952009000600011.xml\n",
      "Description: Dyslipidaemia is a well-established risk factor for cardiovascular diseases in the general population. However, this association is not observed in chronic kidney disease (CKD) patients. This study examines the association between lipid levels, including apolipoproteins A-I and B concentrations, and all-cause mortality or the development of new cardiovascular events in advanced CKD patients. This observational prospective historical study included 331 patients with CKD stage 4 or 5 not yet on dialysis. In addition to conventional clinical and biochemical data, total cholesterol, triglycerides, HDL, LDL, apolipoprotein A-I (apo A) and B (apo B) plasma concentrations were measured. Cox proportional hazard models were adjusted for age, sex, comorbidity index, residual renal function, serum albumin, C-reactive protein levels, and treatment with statins. The median follow-up time was 985 days, and during this period 105 patients died and 54 patients had a new cardiovascular event. In fully-adjusted fixed-covariate Cox models, the hazard ratio for each 10mg/dl increase of apo A concentration was 0.915 (C.I. 95% 0.844 to 0.992; p = 0,031). Patients with an apo A/apo B ratio in the upper tertile (i.e. >1.42) had a better survival rate than the rest of the study patients (hazard ratio = 0.592, C.I. 95% 0.368 to 0.953, p < 0.05). None of the study lipid parameters were associated with new cardiovascular events in the adjusted models. In conclusion, apo A concentrations and high apo A/apo B ratios added independent predictive information about survival of CKD patients not yet on dialysis.\n",
      "File: S0211-69952009000500015.xml\n",
      "Description: We present two cases of Strongyloides stercoralis infection in renal transplant recipients in our centre. We describe clinical presentation characteristics, treatment and resolution.\n",
      "File: S0211-69952009000500009.xml\n",
      "Description: Objectives: To determine the frequency and type of thyroid dysfunction in children with chronic renal failure (CRF) in peritoneal dialysis (PD) or hemodialysis (HD); and to establish the accuracy of the presence of goiter to identify patients with CRF and thyroid dysfunction. Patients and methods: This is a crosssectional study performed in a tertiary pediatric medical care center. CRF patients younger than 17 years old, with more than three months in PD or HD were included. All patients were assessed regarding their growth and sexual development; thyroid dysfunction was evaluated by serum concentration of thyrotropin (TSH), thyroxine (T4L) and triiodothyronine (T3T). Results: 50 patients were included, 25 were male, and mean age was 13 years old. There were 14 (28%) patients with thyroid dysfunction; nine had subclinical hypothyroidism, three patients had euthyroid sick syndrome and two primary hypothyroidism. Thirteen patients had goiter: seven had thyroid dysfunction and in six patients the thyroid function was normal. The sensitivity of goiter to detect thyroid dysfunction was 50% and the specificity was 83.3%. The two patients with the greatest delay in their growth were hypothyroid. Conclusions: Given the high frequency of thyroid dysfunction in children with CRF, these patients need a systematic screening, in order to improve their quality of care.\n",
      "File: S0211-69952009000500002.xml\n",
      "Description: Connective tissue growth factor (CTGF) is increased in several pathologies associated with fibrosis, including multiple renal diseases. CTGF is involved in biological processes such as cell cycle regulation, migration, adhesion and angiogenesis. Its expression is regulated by various factors involved in renal damage, such as Angiotensin II, transforming growth factor-beta, high concentrations of glucose and cellular stress. CTGF is involved in the initiation and progression of renal damage to be able to induce an inflammatory response and promote fibrosis, identified as a potential therapeutic target in the treatment of kidney diseases. In this paper we review the main actions of CTGF in renal disease, the intracellular action mechanisms and therapeutic strategies for its blocking.\n",
      "File: S0211-69952009000500007.xml\n",
      "Description: Background: Obesity increases the risk of proteinuria and chronic renal insufficiency and hastens the progression of renal diseases. Increased activity of renin-angiotensin-aldosterone system and elevated levels of aldosterone are common in obese patients. No studies have compared the efficacy of the currently available antiproteinuric strategies (ACE inhibitors -ACEI-, angiotensin receptor blockers -ARB-, aldosterone antagonists) in obese patients with proteinuric renal diseases. Methods: Single centre, prospective, randomized study. Twelve obese patients (body mass index >30 Kg/m²) with proteinuria >0.5 g/24 h were selected from our outpatient renal clinic. Patients were consecutively treated during 6 weeks with an ACEI (lisinopril 20 mg/day), combined therapy ACEI + ARB (lisinopril 10 mg/day + candesartán 16 mg/day) and eplerenone (25 mg/day) in random order. A drug washout period of 6 weeks was established between the different treatment periods. The primary outcome point was the change in 24-h proteinuria at the end of each treatment period and the number of patients showing a proteinuria reduction greater than 25% of baseline. Results: The reduction in proteinuria induced by lisinopril (11.3 ± 34.8%) was not statistically significant with respect to baseline, whereas that of lisinopril plus candesartán (26.9 ± 30.6%) and eplerenone (28.4 ± 31.6%) showed a statistically significant difference both with respect to baseline values and to lisinopril group. The number of patients who showed a greater than 25% proteinuria reduction was significantly higher with eplerenone (67%) and lisinopril+candesartán (67%) than with lisinopril (25%). Conclusions: Monotherapy with an aldosterone antagonist and combination therapy with ACEI + ARB were more effective than ACEI monotherapy to reduce proteinuria in obese patients with proteinuric renal diseases.\n",
      "File: S0211-69952009000600003.xml\n",
      "Description: Peritonitis is one of the most serious complications of peritoneal dialysis. Pathogenic bacteria cause the majority of cases of peritonitis. Fungal infection is rare but it is associated with high morbidity, the inability to continue on the dialysis program and a high mortality rate. Its incidence ranges from 4% to 10% of all peritonitis episodes in children and from 1% to 23% in adults. Its clinical presentation is similar to bacterial peritonitis. Until now, predisposing factors of fungal peritonitis have not been clearly established; the history of bacterial peritonitis episodes and treatment with broad-spectrum antibiotics have been often reported in literature. Candida species were the most common pathogens and Candida albicans was the most frequent, but high prevalence of Candida parapsilosis has been observed in the last decade. Microbiological findings are essential to determine the etiology of peritonitis. Successful management of fungal peritonitis requires antifungal therapy, the removal of the peritoneal catheter and the subsequent transfer to hemodialysis. Fluconazole and amphotericin B are recommended as antifungal agents. New drugs such as voriconazole and caspofungin are very effective. The aim of this systematic review has been to analyse the clinical and microbiological aspects of fungal peritonitis, as they are not well known and have changed in the last few years.\n",
      "File: S0211-69952009000500010.xml\n",
      "Description: Aims: To study the features of acute renal failure (ARF) in our hospital and to determine prognosis and mortality associated factors. Methods: This is a retrospective study analyzing the ARF episodes observed in our center during a two years period (2005-2007). ARF was considered when a sudden rise in serum creatinine concentration was more than 0, 5 mg/dl in patients with normal renal function and more than 1 mg/dl in patients with previous mild to moderate chronic renal failure. We analyzed epidemiologic, clinical, laboratories results, therapeutics and prognosis factors. Results: Two hundred and one patients were evaluated (62, 7% males; Age= 67,35 ± 16,38 years (63,68% >65 años); Comorbility Index of Charlson was 3,49 ± 2,43). 115 ARF episodes occurred in patients with previous renal failure. ARF was pre-renal in 52,7%, renal in 34,8% and post-renal in 8,5%. 35,8% of ARF patients had oliguria or anuria. The mean duration of ARF/hospitalization was 22,47 days (22,47 ± 21,83). The percentage of resolved ARF was 70, 1%. Mortality was 30,8%. The univariate analysis showed that comorbility Index of Charlson, oliguria, low serum albumin, low cholesterol and anemia were significantly associated with mortality (p < 0,05). However, only Charlson Index, oliguria and low serum cholesterol were independent predictors of mortality in multivariate analysis. Mortality predictive model was carried out. Conclusion: Highest basal comorbility of patients, oliguria and malnutrition inflamation dates are independent predictors of mortality in patients with acute renal failure.\n",
      "File: S0211-69952009000600006.xml\n",
      "Description: In June 2009, WHO declared pandemic swine origin influenza A virus (H1N1). Since then, nephrologists were involved in several of our activities. Decreased work attendance of healthcare for chronic renal patients. Appeared new cases of acute renal failure associated with viral infection, mostly in young adults, with high mortality rate. There were infections in renal transplant patients and temporarily decreased the organs procurement in the weeks of further spread. Scientific institutions were mobilized to agree on protocols for assessment and treatment in order to lessen the impact of the pandemic in renal patients.\n",
      "File: S0211-69952009000600004.xml\n",
      "Description: Magnesium is the fourth-most abundant cation in the human body and the second-most abundant intracellular cation after potassium. Magnesium is pivotal in the transfer, storage, and utilization of energy as it regulates and catalyzes more than 300 enzyme systems. Hypomagnesemia may thus result in a variety of metabolic abnormalities and clinical consequences. It results from an imbalance between gastrointestinal absorption and renal excretion of magnesium. The main consequence related directly to hypomagnesemia is cardiovascular arrhythmias secondary to hipokaliemia and if this is not recognized and treated it may be fatal. In this article we review the hypomagnesemic disorders in children with emphasis on the molecular mechanisms responsible for abnormalities in magnesium homeostasis, differential diagnosis and appropriate therapy, and we describe the clinical and biochemical manifestations as well as the genetic defect in a family with Gitelman syndrome.\n",
      "File: S0211-69952009000600010.xml\n",
      "Description: Objective: To estimate the health related quality of life in patients with chronic kidney disease without dialysis or transplant and your association with risk factors. Design: It was a descriptive transversal study from a representative sample belongs to two Health Insurance Organizations. We applied the medical outcomes study 36-item short form and the scores were associated with some demographics and clinics variables. Results: The median age was 70 years, 67% were men, 93% had hypertension and 67% were in stage three. The physical aspects of quality of life were more affected than mental components (Wilcoxon p <0.001) and the physical functioning, role-physical and body pain domains were better in younger men. In addition, the physical component was most associated with social-demographics and clinics conditions than mental component. Women older than 65 years old with chronic kidney disease and diabetes mellitus obtained lowest scores among all patients. There was no association between glomerular filtration rate and physical health when we fit them by age. There was a significant difference between physical component of quality of life by sex (p <0.001), which 12.5% of variance was explained by age. Conclusion: Physical component of quality of life was significantly reduced compared with mental component among patient with chronic kidney disease without dialysis and transplant. Their scores were lower than general population. The oldest women were the most affected.\n",
      "File: S0211-69952009000500013.xml\n",
      "Description: Aim: To assess gingival overgrowth prevalence and severity in a group of kidney transplant (KT) patients, and analyze the effect of immunosuppressor drugs Cyclosporin A (CyA), Tacrolimus (Tac), Sirolimus (Siro), Azathioprine (Aza) and Mofetil Mycophenolate on this complication. Methods: Gingival overgrowth presence and severity was classified, and the impact of immunosuppressor drugs, age, oral hygiene, verapamil and nifedipine on this condition was analyzed by multiple logistic regression. Results: 172 KT pts. were examined; 137 used CyA, 25 Tac, 6 Sirolimus, 107 Aza and 56 MMF. Gingival overgrowth prevalence was 59.1% on CyA, 12.0% on Tac, and 16.7% on Sirolimus. CyA odds ratio (OR) 15.2, age < 45 OR 5.6, and poor oral hygiene OR 3.2 increased, and Aza OR 0.05 and MMF OR 0.03 decreased GO prevalence. Conclusions: Aza and MMF effect was a significant protection against GO in this group of KT patients.\n",
      "File: S0211-69952009000500011.xml\n",
      "Description: Introduction: Outcome of renal transplant from expanded criteria donors (ECD) is usually inferior than those from standard criteria donors (SCD) and may be improved decreasing cold ischemia time (CIT) and minimizing preservation injury. We compare the results obtained with CIT < 15 hours in kidney transplants from ECD vs. SCD. Subjects and Methods: Prospective, single center study of kidney transplants performed since June 2003 to December 2007. Minimum follow-up period was 12 months. Data of donors, receptors and transplant outcome from ECD and SCD are compared. Results: CIT (mean ± SD) was 9.3 ± 2.5 hours in transplants from ECD (n = 24) and 8.3 ± 3.3 hours in those from SCD (N = 50), p = 0.18. We did not find significant differences among recipients of grafts from ECD and those from SCD regarding: primary non-function (4.2% vs. 2%, respectively), delayed graft function (16.7% vs. 10%), surgical complications (25% vs. 16%) or acute rejection episodes (8.3% vs. 2%). Glomerular filtration rate at one year follow-up was 65.8 ± 14.9 ml/min in ECD recipients and 49.4 ± 12.5 ml/min (p < 0.0001). One year graft survival was 95.8% in ECD recipients and 94% in SCD recipients (p = 0.75). Conclusions: Short CIT in kidney transplant from ECD leads to similar outcome than that obtained from SCD, although renal function is inferior in ECD grafts.\n",
      "File: S0211-69952009000600012.xml\n",
      "Description: We evaluate the incidence of acute rejection, oportunistic infections and non-dermatological malignancies, graft and recipient survival between a group of high immunological risk renal transplant recipients and a group of patients without immunological risk, who received grafts from the same cadaveric donors since 2001 to 2006. This is a prospective and observational study. The risk group (n = 50) included patients with high rate of antibodies (>50%), recipients who had lost their first graft due to early rejection, cross match positive, black race or important histoincompatibility. They received thymoglobulin to mantain T-cell around 10 cells/µl, FK 506 after five days, mycophenolate mofetyl and steroids, with ganciclovir prophylaxis for CMV. The normal risk group (n = 50), cyclosporine, mycophenolate mofetil and steroids. Recipients who lost their graft due to technical failure were excluded. All CMV seronegative recipients who received seropositive grafts were treated with valganciclovir for 100 days. The mean follow-up was 42.7 months. Both groups were similar respect to donor and recipient gender and age, HLA incompatibility, but the percentage of patients with high rate of performed antibodies and second transplant recipients was higher in the high risk group according to the criteria of the study The incidence of acute rejection histologically diagnosed was higher in the normal risk group (30% against 6 %, p = 0.03). There was no difference in opportunistic infections or malignancies, although 2 recipients of the normal risk group developed lymphoproliferative disorders. The recipients survival was 97.9% at 1 and 3 years in both groups, and the graft survival was 89.8% and 84.8% in the high risk group against 93.8% and 90.4% at 1 and 3 years in the normal group (p = NS). We conclude that the evolution of high risk renal transplant recipients is similar to normal risk patients if a potent enough immunosuppression is used. The incidence of acute rejection was higher in the normal risk group\n",
      "File: S0211-69952010000100008.xml\n",
      "Description: Background: Darbepoetin alfa (DA) administered every-other-week (Q2W) is efficacious and safe for the treatment of anaemia in patients undergoing dialysis. There are no data available regarding the evolution of erythropoietic resistance index (ERI) after conversion from weekly (QW) to Q2W administration of DA in clinical practice. Material and methods: Multicenter, observational, retrospective, 16-weeks study, which included stable patients undergoing dialysis who were converted from DA QW to DA Q2W in clinical practice. Conversion was done according to product specifications (duplicating QW dose). The ERI to DA was calculated by dividing the weekly DA dose per kilogram of weight (&micro;g/wk.kg)*200 by the Hb level (g/dL). ERI evolution with time was evaluated by multivariate repeated measures ANOVA, adjusting for significant covariates. Results: A total of 202 patients were included (137 patients undergoing haemodialysis [HD], intravenous (IV) DA, and 65 patients receiving peritoneal dialysis [PD], subcutaneous DA). Mean (SD) age was 66 (17) years; 61% of patients were men. Large intercentre variability was observed for the ERI at conversion time (coefficient of variation of 88%, p <0.001 for differences between centres). In the univariate analysis, predictor factors for high baseline ERI were low albumin level (r = -0.29; p =0.001), HD (mean ERI of 9.3 [8.4] vs 6.8 [4.6] for PD; p = 0.005), or previous cardiovascular disease (9.9 [8.7] vs 7.4 [6.3] for patients without history; p =0.025). During the follow up, the ERI was slightly increased in HD patients (9.3 [8.4] at conversion vs 11.1 [7.3] at 16 weeks; p <0.05), and remained stable in PD patients (6.8 [4.6] vs 6.7 [4.0], respectively; NS). In the multivariate analysis, there were no significant differences in ERI during the 16 weeks post-conversion after adjusting for albumin levels and centre (adjusted baseline mean [95% CI] of 10.0 [8.7-11.4] vs 10.5 [9.3-11.8] at 16 weeks, adjusted change of +0.5 [-0.67; 1.67] ; NS). After 16 weeks, only 7 patients (3.5%) had discontinued Q2W administration. Conclusions: Extension from weekly to once every-other-week darbepoetin alfa allows to simplify anaemia treatment without increasing the resistance index, regardless of dialysis type. The multivariate analysis shows that, after adjusting by center and inflammation/nutritional status, there were no changes in the response to darbepoetin alfa during the first 16 weeks after conversion in clinical practice.\n",
      "File: S0211-69952010000100007.xml\n",
      "Description: Introduction: During the last years the number of patients on waiting list for kidney transplantation has been stable. Living donor kidney transplantation is nowadays a chance to increase the pool of donors. However, there are a group of patients with ABO incompatibility, making impossible the transplant until now. The aim of the present study is to describe the experience of Hospital Clinic Barcelona on ABO incompatible living transplantation. Material and methods: A retrospective-descriptive study was made based on 11 living donor kidney recipients with ABO incompatibility in Hospital Clinic of Barcelona from October'06 to January'09. Selective blood group, antibody removal with specific immunoadsortion, immunoglobulin and anti-CD20 antibody were made until the immunoglobulin (IgG) and isoaglutinine (IgM) antibody titters were 1/8 or lower. Immunosuppressive protocol was adjusted to particular recipient characteristics. Isoaglutinine titters were set before, during and post desensitization treatment and two weeks after transplant. Immunological, medical and surgical evaluation was the standard in living donor kidney transplant program. Results: Medium age of donors and recipients were 47.8 &plusmn;12.4 and 44.4 &plusmn; 14.1 years, respectively. 90% of donors were females and 73% of recipients males. Follow-up time was 10.2 &plusmn;10.2 months. Siblings and spouses were the most frequent relation (n = 4, 36.4%, respectively). Chronic glomerulonephritis, adult polycystic kidney disease and Alport syndrome, the most frequent cause of end-stage renal disease. All the patients acquire appropriate isoaglutinine titters pre transplant (<1/8), requiring 5.54 &plusmn; 2.6 immunoadsorption sessions pretransplant and 2.82 postransplant. One patient didn't need any immunoadsorption session (incompatibility blood group B) and another patient plasma exchange instead of immunoadsorption for being hipersensitized with positive flow cytometry crossmath. Postransplant isoaglutinine titters remained low. Two patients had cellular acute rejection episode (type IA and IB of Banff classification) with good response to corticosteroid treatment. Patient and graft survival were 91% at first year and remain stable during the follow-up. A graft lost by death of patient in relation to haemorrhagic shock developed within the first 72 hours after transplantation. Renal graft function at first year was excellent with serum creatinina of 1.3 &plusmn; 0.8 mg/dl, creatinine clearance of 62.6 ml/min/1.73 m² and proteinuria of 244.9 mg/U 24 h. Conclusion: ABO incompatible living donor kidney transplantation represent an effective and safe alternative in certain patients on waiting list for renal transplant, obtaining excellent results in patient and graft survival, with good renal graft function.\n",
      "File: S0211-69952010000100006.xml\n",
      "Description: In this study we show the results derived from the processing of the data of the Registry of the patients on peritoneal dialysis that initiated renal replacement therapy in Andalucía between January of 1999 and December of 2008. All the information comes from the base of the Registry of Renal Patients of the Andalucia's Health Service. The results show demographic data, distribution by provinces, etiology of the end stage renal disease, reason for election of the peritoneal dialysis, inclusion or not in list of renal transplant, catheter data, withdraws and their causes, and peritonitis data of 2008. We also analyze in the report, from 1999-2008: anual incidence, diabetes, automatic peritoneal dialysis and peritonitis incidence. Finally we have studied patient and technique survival and factors affecting mortality on peritoneal dialysis, the initial comorbid conditions and its impact in the patient's survival.\n",
      "File: S0211-69952010000100004.xml\n",
      "Description: Associated renal and cardiac diseases have a high prevalence among the population in several clinical contexts: acute renal failure in the context of decompensated heart failure (HF), HF patients who develop chronic kidney disease (CKD) and patients with CKD who develop HF. In recent years, cardiorenal syndrome has been described as deteriorating kidney function in the context of HF. However, there are other clinical situations for which nephrologists can contribute their knowledge as a part of an integral treatment strategy, as is the case with refractory HF (RHF). All of these situations require an interdisciplinary cooperative effort between cardiologists and nephrologists with the aim of providing integral treatment. This article aims to review the role of the nephrologist in HF treatment, with an emphasis on the subgroup of patients with RHF and current evidence regarding the usefulness of peritoneal dialysis (PD) as a chronic coadjuvant treatment.\n",
      "File: S0211-69952009000600015.xml\n",
      "Description: Abstract Chronic myeloid leukemia (CML) is a myeloproliferative disorder characterized by clonal expansion of cells in the myeloid line, expressing the BCRABL fusion protein responsible for the oncogenic effect of CML. The current frontline therapy in CML is the BCR-ABL tyrosine kinase inhibitor, Imatinib. Although this drug has been shown to improve survival in CML patients, its role in the context of a transplant setting has not been widely described in the literature. We report on the long term molecular remission of CML in a 55 year old man with a second renal transplant who is hepatitis C virus positive, and has associated cardiovascular and immunological risk factors.\n",
      "File: S0211-69952009000600014.xml\n",
      "Description: Introduction: Slit diaphragm and/or podocyte's cytoskeleton alterations are related to proteinuria and nephrotic syndrome. In our population, focal and segmental glomerulosclerosis causing nephrotic syndrome is the more frequent biopsy demonstrated glomerulopathy. Our aim was search for alterations in some slit diaphragm-associated proteins in patients with nephrotic range proteinuria. Methods: Renal tissue from 40 patients with nephrotic range proteinuria, 10 patients with non-nephrotic proteinuria, 3 with isolated hematuria, and 10 samples of normal renal tissue (deceased donors) were studied, by indirect immunofluorescence, for expression of nephrin, podocin, and alpha-actinin-4. Results: Expression of these proteins was lineal, homogeneous, in the glomerular capillary walls in normal renal tissue and in patients with isolated hematuria. In nephrotic proteinuria this normal appearance was altered and immunostaining showed a fine granular appearance. In 18 cases (45%) of patients with nephrotic proteinuria and 3 cases (30%) of patients with non-nephrotic proteinuria there was loss of at least one of these proteins (p = 0.49). These alterations were found in the diverse glomerulopathies more frequently causing nephrotic syndrome. Conclusions: In nephrotic range proteinuria redistribution or loss of expression of slit diaphragm-associated proteins is very frequent. In many of our cases this fact could be more a consequence than a cause of proteinuria. These alterations can be also evidenced in patients with non-nephrotic proteinuria.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK-cub2Nhqmv"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Create a function that splits the texts into sentences, using the sent_tokenize function. The function should display the average number of sentences per file analyzed, the name of the file containing the fewest sentences, and the name of the file containing the most sentences."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "id": "0psvWnpYXAV5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739803357080,
     "user_tz": -60,
     "elapsed": 180,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for item in file_data:\n",
    "    text = item['description']\n",
    "    sentences = sent_tokenize(text, language=\"english\")\n",
    "    item['sentence_count'] = len(sentences)"
   ],
   "metadata": {
    "id": "zjS-bdj6XIfL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739805607089,
     "user_tz": -60,
     "elapsed": 176,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "total_sentences = sum(item[\"sentence_count\"] for item in file_data)\n",
    "average_sentences = total_sentences / len(file_data)\n",
    "\n",
    "min_file = min(file_data, key=lambda item: item[\"sentence_count\"])\n",
    "max_file = max(file_data, key=lambda item: item[\"sentence_count\"])"
   ],
   "metadata": {
    "id": "Q73lT8xffutU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739805728123,
     "user_tz": -60,
     "elapsed": 172,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Average amount of sentences in file: {average_sentences}\")\n",
    "print(f\"File with fewest sentences: {min_file['filename']} with {min_file['sentence_count']} sentences\")\n",
    "print(f\"File with most sentences: {max_file['filename']} with {max_file['sentence_count']} sentences\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8td3jZ_gPHo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739805831951,
     "user_tz": -60,
     "elapsed": 231,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "b3cc4cd2-2771-405c-bbb6-8235151a4d89"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average amount of sentences in file: 10.84\n",
      "File with fewest sentences: S0211-69952009000500015.xml with 2 sentences\n",
      "File with most sentences: S0211-69952009000500008.xml with 24 sentences\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NehqwQRAhnWI"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a program that splits the texts into sentences. Subsequently, perform word tokenization using the `WordPunctTokenizer` class. Finally, the function should display the average number of words per file, the file containing the fewest words, and the file containing the most words."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import WordPunctTokenizer"
   ],
   "metadata": {
    "id": "q_YFvOY_gK1m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806164570,
     "user_tz": -60,
     "elapsed": 181,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ],
   "metadata": {
    "id": "x8bNnSWcXHv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806174420,
     "user_tz": -60,
     "elapsed": 174,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for item in file_data:\n",
    "    text = item['description']\n",
    "    sentences = sent_tokenize(text, language=\"english\")\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend(tokenizer.tokenize(sentence))\n",
    "    item['word_count'] = len(words)"
   ],
   "metadata": {
    "id": "ZAQtJsIfh36U",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806566393,
     "user_tz": -60,
     "elapsed": 155,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "total_words = sum(item[\"word_count\"] for item in file_data)\n",
    "average_words = total_words / len(file_data)\n",
    "\n",
    "min_file = min(file_data, key=lambda item: item[\"word_count\"])\n",
    "max_file = max(file_data, key=lambda item: item[\"word_count\"])"
   ],
   "metadata": {
    "id": "y8x4a68KjYJ4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806617537,
     "user_tz": -60,
     "elapsed": 158,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Average amount of words in file: {average_words}\")\n",
    "print(f\"File with fewest words: {min_file['filename']} with {min_file['word_count']} words\")\n",
    "print(f\"File with most words: {max_file['filename']} with {max_file['word_count']} words\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esSaS6N1jnph",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806674581,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "7369dd9c-d15a-439a-e7df-021a96b874e2"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average amount of words in file: 294.08\n",
      "File with fewest words: S0211-69952009000500015.xml with 26 words\n",
      "File with most words: S0211-69952009000500012.xml with 666 words\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUey1NQvjBvt"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Split the sentence shown below into tokens using the following tokenizers: TreebankWordTokenizer, WhitespaceTokenizer, SpaceTokenizer, and WordPunctTokenizer from NLTK and gpt-4o-mini from Tiktoken.\n",
    "\n",
    "You have to explain the differences among the tokenizers with the following sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GKGF_rnvGxZx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806772835,
     "user_tz": -60,
     "elapsed": 187,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Sorry, I can't go to the meeting.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "TreebankWordTokenizer = TreebankWordTokenizer()\n",
    "print(TreebankWordTokenizer.tokenize(sentence))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSL2A9I5kv1N",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806963234,
     "user_tz": -60,
     "elapsed": 188,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "110fab6a-e504-465c-8c6a-a14316dd1e32"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sorry', ',', 'I', 'ca', \"n't\", 'go', 'to', 'the', 'meeting', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import WhitespaceTokenizer\n",
    "WhitespaceTokenizer = WhitespaceTokenizer()\n",
    "print(WhitespaceTokenizer.tokenize(sentence))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3I-KaR_Sk4Py",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806986419,
     "user_tz": -60,
     "elapsed": 194,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "64bbde0e-0edd-4555-bc7b-425caab61f1c"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sorry,', 'I', \"can't\", 'go', 'to', 'the', 'meeting.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import SpaceTokenizer\n",
    "SpaceTokenizer = SpaceTokenizer()\n",
    "print(SpaceTokenizer.tokenize(sentence))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGb9yPQDlAE2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739807019803,
     "user_tz": -60,
     "elapsed": 181,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "ea1334db-e422-4875-f0c8-28b2bb822052"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sorry,', 'I', \"can't\", 'go', 'to', 'the', 'meeting.\\n']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "WordPunctTokenizer = WordPunctTokenizer()\n",
    "print(WordPunctTokenizer.tokenize(sentence))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZETXQRk6kFRL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739806919043,
     "user_tz": -60,
     "elapsed": 189,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "b298d6dd-9cc6-4519-88bf-6f99d5aa5e73"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sorry', ',', 'I', 'can', \"'\", 't', 'go', 'to', 'the', 'meeting', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tiktoken"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WClPqyrnlbRE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739807115543,
     "user_tz": -60,
     "elapsed": 3177,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "e7cab3a6-aef6-425a-eef8-d7b88e32fc45"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "encoded_text = encoding.encode(sentence)\n",
    "print(encoded_text)\n",
    "print([encoding.decode_single_token_bytes(token) for token in encoded_text])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtkIv0HukufT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739807696137,
     "user_tz": -60,
     "elapsed": 199,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "f66f33e8-11f1-4a16-f83c-12bb52206d50"
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[33680, 11, 357, 8535, 810, 316, 290, 9176, 558]\n",
      "[b'Sorry', b',', b' I', b\" can't\", b' go', b' to', b' the', b' meeting', b'.\\n']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explanation of Tokenizer Differences\n",
    "\n",
    "**TreebankWordTokenizer:** Splits contractions (e.g., \"can't\" -> \"ca\", \"n't\"). Keeps punctuation attached.\n",
    "\n",
    "**WhitespaceTokenizer:** Splits on all whitespace (spaces, tabs, newlines). Keeps punctuation attached.\n",
    "\n",
    "**SpaceTokenizer:** Splits only on spaces. Other whitespace (like \\n) is part of the token.\n",
    "\n",
    "**WordPunctTokenizer:** Splits all punctuation separately. Splits contractions.\n",
    "\n",
    "**Tiktoken (gpt-4o-mini):** Uses Byte Pair Encoding (BPE). Handles contractions as single tokens. Adds leading spaces to some tokens.  Newlines are separate. Designed for LLMs.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "*   **Contractions:** Treebank & WordPunct split. Whitespace & Space keep together. Tiktoken keeps together (but can vary).\n",
    "*   **Punctuation:** WordPunct separates. Others attach.\n",
    "*   **Whitespace:** Whitespace splits on all. Space only on spaces. Tiktoken handles contextually.\n",
    "*   **Subwords:** Tiktoken uses subwords. Others don't."
   ],
   "metadata": {
    "id": "PY0TWQWfmQoK"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8JoaOaIjmGD"
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Create a tokenizer based on regular expressions using the RegexpTokenizer class from NLTK that extracts only the words present in the text, meaning it should not return punctuation marks, tabs/line breaks, etc., as output.\n",
    "\n",
    "Furthermore, the tokenizer should not separate contractions in the text.\n",
    "\n",
    "What are the tokens extracted if we pass the following sentence to it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BkBxs28RHGRo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739808074205,
     "user_tz": -60,
     "elapsed": 196,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Sorry, I can't go to the meeting.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:'[a-z]+)?\")\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mgl5QHEGpHva",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739809215891,
     "user_tz": -60,
     "elapsed": 208,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "af155637-4bb3-46ef-cfef-cc16bcfa97fd"
   },
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'to', 'the', 'meeting']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ylmj6CIWdkqB"
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "Using the SFU corpus, composed of 400 opinion documents from 8 different domains (books, cars, computers, kitchen utensils, hotels, movies, and phones), the following operations should be performed:\n",
    "\n",
    "**Note**: The corpus is located in the \"Material Complementario\" section of PLATEA.\n",
    "\n",
    "* Show the size of the vocabulary (unique tokens) of each domain (using 2 tokenizers, one of them based on BPE).\n",
    "* Show the total number of stop words per domain (using 2 tokenizers, one of them based on BPE).\n",
    "* Show the percentage of stop words in relation to the number of unique tokens and unique words (without punctuation marks; using 2 tokenizers, one of them based on BPE).\n",
    "* Show the 5 most common stems in each domain, obviously without considering stop words (using 2 tokenizers, one of them based on BPE).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk import TreebankWordTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "domains = []\n",
    "path = \"/content/drive/MyDrive/NLP/SFU\"\n",
    "\n",
    "for item in os.listdir(path):\n",
    "  item_path = os.path.join(path, item)\n",
    "  if os.path.isdir(item_path):\n",
    "    domains.append(item)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "bpe_tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wShDq0LMFljl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739815643483,
     "user_tz": -60,
     "elapsed": 404,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "18cc3189-317f-4818-b822-2f71b60f5759"
   },
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_domain(domain_path, tokenizer, bpe_tokenizer):\n",
    "    all_words = []\n",
    "    bpe_words = []\n",
    "\n",
    "    for file in os.listdir(domain_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(domain_path, file)\n",
    "            with open(file_path, \"r\", encoding='ascii', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "                words = tokenizer.tokenize(text)\n",
    "                all_words.extend(words)\n",
    "                bpe_tokens = bpe_tokenizer.encode(text)\n",
    "                bpe_decoded_tokens = [bpe_tokenizer.decode([token]) for token in bpe_tokens]\n",
    "                bpe_words.extend(bpe_decoded_tokens)\n",
    "\n",
    "    return all_words, bpe_words\n",
    "\n",
    "def statistics(words, bpe_words, domain):\n",
    "  # Show the size of the vocabulary (unique tokens) of each domain\n",
    "  vocabulary = set(words)\n",
    "  bpe_vocabulary = set(bpe_words)\n",
    "\n",
    "  # Show the total number of stop words per domain\n",
    "  stop_words_count = 0\n",
    "  for word in vocabulary:\n",
    "    if word in stop_words:\n",
    "      stop_words_count += 1\n",
    "\n",
    "  bpe_stop_words_count = 0\n",
    "  for word in bpe_vocabulary:\n",
    "    if word in stop_words:\n",
    "      bpe_stop_words_count +=1\n",
    "\n",
    "  # Show the percentage of stop words in relation to the number of unique tokens and unique words (without punctuation marks)\n",
    "  vocabulary_no_punct = []\n",
    "  for w in vocabulary:\n",
    "    if w.isalnum():\n",
    "      vocabulary_no_punct.append(w)\n",
    "\n",
    "  bpe_vocabulary_no_punct = []\n",
    "  for w in bpe_vocabulary:\n",
    "    if w.isalnum():\n",
    "      bpe_vocabulary_no_punct.append(w)\n",
    "\n",
    "  stop_words_percentage = (stop_words_count / len(vocabulary_no_punct)) * 100 if vocabulary_no_punct else 0\n",
    "  bpe_stop_words_percentage = (bpe_stop_words_count / len(bpe_vocabulary_no_punct)) * 100 if bpe_vocabulary_no_punct else 0\n",
    "\n",
    "  # Show the 5 most common stems in each domain, without considering stop words\n",
    "  stemmed_words = []\n",
    "  for w in vocabulary_no_punct:\n",
    "    if w not in stop_words:\n",
    "        stemmed_w = stemmer.stem(w)\n",
    "        stemmed_words.append(stemmed_w)\n",
    "\n",
    "  bpe_stemmed_words = []\n",
    "  for w in bpe_vocabulary_no_punct:\n",
    "    if w not in stop_words:\n",
    "        stemmed_w = stemmer.stem(w)\n",
    "        bpe_stemmed_words.append(stemmed_w)\n",
    "\n",
    "  stem_counts = Counter(stemmed_words)\n",
    "  bpe_stem_counts = Counter(bpe_stemmed_words)\n",
    "\n",
    "  most_common_stems = stem_counts.most_common(5)\n",
    "  bpe_most_common_stems = bpe_stem_counts.most_common(5)\n",
    "\n",
    "\n",
    "  print(f\"Domain: {domain}\")\n",
    "  print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "  print(f\"Vocabulary size (BPE): {len(bpe_vocabulary)}\")\n",
    "  print(f\"Stop words count: {stop_words_count}\")\n",
    "  print(f\"Stop words count (BPE): {bpe_stop_words_count}\")\n",
    "  print(f\"Stop words percentage: {stop_words_percentage}%\")\n",
    "  print(f\"Stop words percentage (BPE): {bpe_stop_words_percentage}%\")\n",
    "  print(f\"Most common stems: {most_common_stems}\")\n",
    "  print(f\"Most common stems (BPE): {bpe_most_common_stems}\")\n"
   ],
   "metadata": {
    "id": "TELIrXlLpHGa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739816279768,
     "user_tz": -60,
     "elapsed": 218,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    }
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(domains)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjTIbxHiESYR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739815994278,
     "user_tz": -60,
     "elapsed": 178,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "7b2a5d32-e969-49b1-cbcb-c4ac987267bd"
   },
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['BOOKS', 'CARS', 'COMPUTERS', 'COOKWARE', 'HOTELS', 'MOVIES', 'MUSIC', 'PHONES']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for domain in domains:\n",
    "    domain_path = os.path.join(path, domain)\n",
    "    all_words, bpe_words = analyze_domain(domain_path, tokenizer, bpe_tokenizer)\n",
    "    statistics(all_words, bpe_words, domain)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdJR22arE7su",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739816288815,
     "user_tz": -60,
     "elapsed": 3644,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "92dce605-e1e1-4b5b-c736-de924b061929"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Domain: BOOKS\n",
      "Vocabulary size: 5553\n",
      "Vocabulary size (BPE): 5760\n",
      "Stop words count: 118\n",
      "Stop words count (BPE): 53\n",
      "Stop words percentage: 2.619893428063943%\n",
      "Stop words percentage (BPE): 4.985888993414863%\n",
      "Most common stems: [('narrat', 8), ('murder', 8), ('like', 7), ('believ', 7), ('develop', 6)]\n",
      "Most common stems (BPE): [('ell', 4), ('ate', 4), ('ere', 4), ('ation', 4), ('ing', 4)]\n",
      "Domain: CARS\n",
      "Vocabulary size: 7939\n",
      "Vocabulary size (BPE): 7460\n",
      "Stop words count: 130\n",
      "Stop words count (BPE): 66\n",
      "Stop words percentage: 2.2146507666098807%\n",
      "Stop words percentage (BPE): 3.9927404718693285%\n",
      "Most common stems: [('acceler', 9), ('impress', 8), ('posit', 8), ('engin', 8), ('adjust', 7)]\n",
      "Most common stems (BPE): [('ate', 5), ('anc', 5), ('age', 5), ('ide', 4), ('ine', 4)]\n",
      "Domain: COMPUTERS\n",
      "Vocabulary size: 7135\n",
      "Vocabulary size (BPE): 6935\n",
      "Stop words count: 118\n",
      "Stop words count (BPE): 65\n",
      "Stop words percentage: 2.1965748324646315%\n",
      "Stop words percentage (BPE): 4.049844236760125%\n",
      "Most common stems: [('connect', 9), ('look', 8), ('upgrad', 8), ('instal', 8), ('oper', 7)]\n",
      "Most common stems (BPE): [('ate', 7), ('at', 4), ('ive', 4), ('en', 4), ('ation', 4)]\n",
      "Domain: COOKWARE\n",
      "Vocabulary size: 4299\n",
      "Vocabulary size (BPE): 4461\n",
      "Stop words count: 118\n",
      "Stop words count (BPE): 53\n",
      "Stop words percentage: 3.486997635933806%\n",
      "Stop words percentage (BPE): 5.941704035874439%\n",
      "Most common stems: [('saut', 8), ('clean', 7), ('conduct', 7), ('heat', 7), ('serv', 7)]\n",
      "Most common stems (BPE): [('ate', 5), ('it', 4), ('ute', 4), ('an', 3), ('cl', 3)]\n",
      "Domain: HOTELS\n",
      "Vocabulary size: 6145\n",
      "Vocabulary size (BPE): 6142\n",
      "Stop words count: 119\n",
      "Stop words count (BPE): 54\n",
      "Stop words percentage: 2.4668325041459367%\n",
      "Stop words percentage (BPE): 4.607508532423208%\n",
      "Most common stems: [('travel', 7), ('shop', 6), ('person', 6), ('tri', 6), ('vacat', 6)]\n",
      "Most common stems (BPE): [('ate', 5), ('one', 4), ('ant', 4), ('ive', 4), ('ie', 4)]\n",
      "Domain: MOVIES\n",
      "Vocabulary size: 6243\n",
      "Vocabulary size (BPE): 6504\n",
      "Stop words count: 123\n",
      "Stop words count (BPE): 61\n",
      "Stop words percentage: 2.443384982121573%\n",
      "Stop words percentage (BPE): 4.6995377503852085%\n",
      "Most common stems: [('hope', 7), ('love', 7), ('play', 7), ('person', 6), ('begin', 6)]\n",
      "Most common stems (BPE): [('one', 6), ('ate', 5), ('ive', 5), ('ack', 5), ('at', 4)]\n",
      "Domain: MUSIC\n",
      "Vocabulary size: 8417\n",
      "Vocabulary size (BPE): 8270\n",
      "Stop words count: 121\n",
      "Stop words count (BPE): 67\n",
      "Stop words percentage: 1.839464882943144%\n",
      "Stop words percentage (BPE): 3.476907109496627%\n",
      "Most common stems: [('listen', 9), ('look', 8), ('lyric', 8), ('love', 7), ('perform', 7)]\n",
      "Most common stems (BPE): [('one', 6), ('ash', 6), ('ate', 5), ('ive', 5), ('ack', 5)]\n",
      "Domain: PHONES\n",
      "Vocabulary size: 2981\n",
      "Vocabulary size (BPE): 3048\n",
      "Stop words count: 115\n",
      "Stop words count (BPE): 39\n",
      "Stop words percentage: 4.901960784313726%\n",
      "Stop words percentage (BPE): 6.63265306122449%\n",
      "Most common stems: [('call', 8), ('answer', 7), ('light', 6), ('sound', 6), ('end', 5)]\n",
      "Most common stems (BPE): [('er', 4), ('set', 3), ('con', 3), ('ing', 3), ('phone', 3)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import chardet\n",
    "\n",
    "with open(\"/content/drive/MyDrive/NLP/SFU/BOOKS/no1.txt\", 'rb') as f:  # Открываем файл в бинарном режиме\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "print(result['encoding'])  # Выводит предполагаемую кодировку"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPU6NzbgGWUQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1739815897954,
     "user_tz": -60,
     "elapsed": 436,
     "user": {
      "displayName": "Yuliya Dabreha",
      "userId": "05622474096039686923"
     }
    },
    "outputId": "450b95f3-9cc1-46a9-fe8e-9a53ccf536ce"
   },
   "execution_count": 64,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ascii\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "lORP-R5cGyre"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2NrEE6fcgEpP",
    "Sg-FkToOhuGQ",
    "Iz37vkmwnK7P",
    "dQikxXZxn6c3",
    "u22w3BFiOveA",
    "DMTAs5WJQE5h",
    "Fuc95vvxPdOv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
