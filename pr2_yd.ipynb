{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"u01d30dm7Uyg"},"source":["# **Natural Language Processing**"]},{"cell_type":"markdown","metadata":{"id":"6TOjFk9n81ZZ"},"source":["## spaCy\n","\n","SpaCy is a natural language processing (NLP) library in Python, designed to be fast, efficient and easy to use in practical applications. It is widely used for tasks such as:\n","\n","* Tokenisation: Breaking text into words, phrases or smaller units.\n","* Lemmatisation: Reducing words to their base form (e.g. running → run).\n","* POS tagging: Identifying the type of each word (noun, verb, adjective, etc.).\n","* Entity recognition (NER): Detect proper names, dates, organisations, etc.\n","* Parsing: Determining the grammatical structure of a sentence.\n","* Word vectorisation: Representing words in numerical form for ML and deep learning models.\n","\n","Unlike NLTK, which is more research and experimentation oriented, SpaCy is optimised for production and efficiency. In addition, it includes pre-trained models for several languages and allows the integration of neural network models (such as transformers) to improve its performance.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l5ClvYu6rTSS"},"source":["We choose the model we need for information processing and download it:"]},{"cell_type":"code","source":["!pip install spacy # Install the library if not preinstalled"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQo5Nt-NDG2u","outputId":"b8023002-312a-40fb-b722-9c7caebcb5b6","executionInfo":{"status":"ok","timestamp":1740410921443,"user_tz":-60,"elapsed":5747,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"]}]},{"cell_type":"code","metadata":{"id":"Ip9ObXT3rDZe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d9cea909-7a44-4e67-a657-1dddb26d5d54","executionInfo":{"status":"ok","timestamp":1740410937745,"user_tz":-60,"elapsed":16300,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["import spacy\n","spacy.cli.download(\"en_core_web_sm\") # Small model (sm) in English. Also available md (medium) and lg (large)."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"markdown","metadata":{"id":"BHqcfnUQtgPL"},"source":["We can now use the downloaded model in our programme as follows:"]},{"cell_type":"code","metadata":{"id":"NvxnipFCq1Ek"},"source":["nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W50rPd_57eaD"},"source":["## Morphological analysis\n","\n","Morphological analysis consists of determining the grammatical form, class or category of each word in a sentence. This process is also known as POS tagging (Part Of Speech tagging). [List of tags](https://universaldependencies.org/u/pos/).\n","\n","Next, let's see how to get the grammatical category of the words in the sentence:\n","\n","1.   First we create a **doc** from the text we want to parse. Doc is a ***spaCy*** object that stores the text and all its annotations.\n","2.   We then iterate through the document to see what ***spaCy*** has analysed.\n","\n"]},{"cell_type":"code","metadata":{"id":"-KQJRiq3u9Ze","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a960d233-1ea3-47c8-a2b0-58e0d554a7ae","executionInfo":{"status":"ok","timestamp":1740410938962,"user_tz":-60,"elapsed":60,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["text = \"Rainfall in Spain falls mainly on the plain without mountains.\"\n","doc = nlp(text)\n","\n","for token in doc:\n","    print(token.text, token.pos_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rainfall NOUN\n","in ADP\n","Spain PROPN\n","falls VERB\n","mainly ADV\n","on ADP\n","the DET\n","plain NOUN\n","without ADP\n","mountains NOUN\n",". PUNCT\n"]}]},{"cell_type":"markdown","metadata":{"id":"f9a3M_bzv-cQ"},"source":["As we can see, spaCy first performs the tokenisation according to the model used and for the specific language. It then parses and annotates the sentence. Statistical models allow ***spaCy*** to make predictions of which tag is most likely to be applied in a given context.\n","\n","A trained model includes data with enough examples so that it can make predictions that generalise across the language; for example, a word following a determiner ‘the’ is likely to be a noun.\n","\n","Linguistic annotations are available as **Token** providing information for each word in the sentence such as:\n","\n","\n","* Text: The original text of the word.\n","* Lemma: The base form of the word.\n","* POS: The label of the word.\n","* Morph: Thorough revision of the above quality by specifying parameters within the category.\n","* Dep: Syntactic dependency, i.e. the relationship between the tokens.\n","* Form: The form of the word: capitalisation, punctuation, digits.\n","* Alpha: Is the token an alphanumeric character?\n","* Stop-word: Is the token part of a stop-list?\n"]},{"cell_type":"code","metadata":{"id":"OYkuiQwWvBRg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e90c37d8-257f-4cd2-d624-2a8222c29a49","executionInfo":{"status":"ok","timestamp":1740410938966,"user_tz":-60,"elapsed":5,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["doc = nlp(\"Apple is considering buying a British company for $1 billion.\")\n","\n","for token in doc:\n","    print(token.text, token.lemma_, token.pos_, token.morph, token.dep_,\n","            token.shape_, token.is_alpha, token.is_stop)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Apple Apple PROPN Number=Sing nsubj Xxxxx True False\n","is be AUX Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin aux xx True True\n","considering consider VERB Aspect=Prog|Tense=Pres|VerbForm=Part ROOT xxxx True False\n","buying buy VERB Aspect=Prog|Tense=Pres|VerbForm=Part xcomp xxxx True False\n","a a DET Definite=Ind|PronType=Art det x True True\n","British british ADJ Degree=Pos amod Xxxxx True False\n","company company NOUN Number=Sing dobj xxxx True False\n","for for ADP  prep xxx True True\n","$ $ SYM  quantmod $ False False\n","1 1 NUM NumType=Card compound d False False\n","billion billion NUM NumType=Card pobj xxxx True False\n",". . PUNCT PunctType=Peri punct . False False\n"]}]},{"cell_type":"markdown","metadata":{"id":"GcqAjAeR5BDD"},"source":["** The form of the words is very simple, a capital X is placed when the letter is in upper case and in lower case in the opposite case, giving structures of Xxxxx also if a number is introduced this also has its own value in which case it is a *d*."]},{"cell_type":"markdown","metadata":{"id":"l2jAoPyEybpJ"},"source":["Most morphological categories seem rather abstract and vary from language to language. The **spacy.explain()** method will show you a brief description in some cases:"]},{"cell_type":"code","metadata":{"id":"fdskGDasyGeP","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b81d1248-d312-4510-ca8c-a51b8abc4efb","executionInfo":{"status":"ok","timestamp":1740410938982,"user_tz":-60,"elapsed":8,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["import spacy\n","spacy.explain(\"DET\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'determiner'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"_fPBzN2JAAPn"},"source":["Syntactic analysis\n","\n","Syntactic analysis consists of determining the syntactic functions or concordance and hierarchical relationships that words have when they are grouped together.\n","\n","The most commonly used types of parsing are constituent parsing and dependency parsing. Constituent analysis consists of dividing the sentence into its component parts (nominal syntagms, verbal syntagms, etc.), which are called constituents, so that each of these parts is in turn divided into smaller parts until we arrive at the words. On the other hand, dependency analysis is based on looking for the relationships between the different words in the sentence.\n","\n","In this session, we are going to carry out a syntactic analysis of dependencies which can be very useful, for example, to determine the subject and the direct complement of a sentence, the words modified by a negator, etc. Its study is important, since the interpretation and comprehension of texts often depend on a correct syntactic analysis.\n","\n","Next, we are going to see how to perform dependency parsing of a sentence using ***spaCy*** again:\n","\n","1. First we must install the package if necessary and download the appropriate model.\n","2.   We create a **doc** from the text we want to parse.\n","3.   Then iterate through the document to see what ***spaCy*** has parsed."]},{"cell_type":"code","metadata":{"id":"-TB886rSVQYc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41bab9c0-7ae6-461f-ff64-147c7223fe73","executionInfo":{"status":"ok","timestamp":1740410946125,"user_tz":-60,"elapsed":7136,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["import spacy\n","spacy.cli.download(\"en_core_web_sm\")\n","nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4c7c011-732d-4627-95c1-dd6ebd800827","id":"DAq9RTU4AAPo","executionInfo":{"status":"ok","timestamp":1740410946156,"user_tz":-60,"elapsed":22,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["text = \"The order has been delivered late.\"\n","doc = nlp(text)\n","\n","for token in doc:\n","    print(token.text, token.pos_, token.dep_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The DET det\n","order NOUN nsubjpass\n","has AUX aux\n","been AUX auxpass\n","delivered VERB ROOT\n","late ADV advmod\n",". PUNCT punct\n"]}]},{"cell_type":"markdown","source":["Dependencies can be represented in a directed graph:\n","\n","* The words are the nodes.\n","* The grammatical relations are the edges.\n","\n","You can use **displacy** to visualise the dependency tree:"],"metadata":{"id":"rxsHqu02_BGA"}},{"cell_type":"code","metadata":{"id":"qsRKVEJTVPNl","colab":{"base_uri":"https://localhost:8080/","height":441},"outputId":"93342611-78ee-4a27-f4bf-0afd8b79cd19","executionInfo":{"status":"ok","timestamp":1740410946208,"user_tz":-60,"elapsed":51,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["from spacy import displacy\n","displacy.render(doc, style='dep', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7018ea2f2f4a4440a80f246404b01b31-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">order</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">has</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">been</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">delivered</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">late.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7018ea2f2f4a4440a80f246404b01b31-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7018ea2f2f4a4440a80f246404b01b31-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7018ea2f2f4a4440a80f246404b01b31-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7018ea2f2f4a4440a80f246404b01b31-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7018ea2f2f4a4440a80f246404b01b31-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7018ea2f2f4a4440a80f246404b01b31-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7018ea2f2f4a4440a80f246404b01b31-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7018ea2f2f4a4440a80f246404b01b31-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-7018ea2f2f4a4440a80f246404b01b31-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-7018ea2f2f4a4440a80f246404b01b31-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"]},"metadata":{}}]},{"cell_type":"code","source":["spacy.explain(\"nsubjpass\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"gCXYiYVDzDNl","outputId":"fb1e0efe-e9a3-4e24-84e7-596dc091f791","executionInfo":{"status":"ok","timestamp":1740410946232,"user_tz":-60,"elapsed":18,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'nominal subject (passive)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"qUkc15ISXte3","colab":{"base_uri":"https://localhost:8080/","height":458},"outputId":"ab0ae37c-c58f-4b70-8d0d-ec430e430fe2","executionInfo":{"status":"ok","timestamp":1740410946486,"user_tz":-60,"elapsed":253,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["text = \"The computer is not good.\"\n","doc = nlp(text)\n","\n","for token in doc:\n","  print(token.text, token.pos_, token.morph, token.dep_, token.head.text)\n","\n","displacy.render(doc, style='dep', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The DET Definite=Def|PronType=Art det computer\n","computer NOUN Number=Sing nsubj is\n","is AUX Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin ROOT is\n","not PART Polarity=Neg neg is\n","good ADJ Degree=Pos acomp is\n",". PUNCT PunctType=Peri punct is\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5bfcb55d4c4f4988ab39ad0d0b074206-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">computer</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">not</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">good.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-5bfcb55d4c4f4988ab39ad0d0b074206-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Navigating the dependency tree**\n","\n","The dependency parsing scheme has the properties of a tree. This tree contains information about sentence structure and grammar and can be traversed in different ways to extract relationships.\n","\n","***SpaCy*** provides attributes such as *children*, *nbor*, *lefts* or *rights* to navigate the dependency tree."],"metadata":{"id":"1T57qrVm_-ID"}},{"cell_type":"code","source":["text = 'bright red apples on the tree'\n","doc = nlp(text)\n","displacy.render(doc, style='dep', jupyter=True)"],"metadata":{"id":"AwwFDfngRVpS","colab":{"base_uri":"https://localhost:8080/","height":353},"outputId":"38e630f2-2edc-4375-ec70-5b7e36b216bf","executionInfo":{"status":"ok","timestamp":1740410946493,"user_tz":-60,"elapsed":12,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"90f3840b02b2428885cb59c5124ea253-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">bright</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">red</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">apples</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">on</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tree</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-90f3840b02b2428885cb59c5124ea253-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-90f3840b02b2428885cb59c5124ea253-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-90f3840b02b2428885cb59c5124ea253-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-90f3840b02b2428885cb59c5124ea253-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-90f3840b02b2428885cb59c5124ea253-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-90f3840b02b2428885cb59c5124ea253-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-90f3840b02b2428885cb59c5124ea253-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-90f3840b02b2428885cb59c5124ea253-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-90f3840b02b2428885cb59c5124ea253-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-90f3840b02b2428885cb59c5124ea253-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"]},"metadata":{}}]},{"cell_type":"code","source":["# Extract the children of ‘apples’.\n","print(f\"Children of {doc[2]}: {list(doc[2].children)}\")\n","\n","# Extract the next neighbour node from `apples`. (default=1)\n","print(f\"Neighbour of {doc[2]}: {doc[2].nbor()}\")\n","\n","# Extract the preceding neighbour node from `red`.\n","print(f\"Preceding neighbour of {doc[1]}: {doc[1].nbor(-1)}\")\n","\n","# Extract the children to the left of `tree`.\n","print(f\"Children to the left of {doc[-1]}: {list(doc[-1].lefts)}\")\n","\n","# Extract children to the right of `apples`.\n","print(f\"Children to the right of {doc[2]}: {list(doc[2].rights)}\")"],"metadata":{"id":"2qPrO5df_9aV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a319634-2acc-4758-cfe3-b28b445597f5","executionInfo":{"status":"ok","timestamp":1740410946501,"user_tz":-60,"elapsed":17,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Children of apples: [bright, red, on]\n","Neighbour of apples: on\n","Preceding neighbour of red: bright\n","Children to the left of tree: [the]\n","Children to the right of apples: [on]\n"]}]},{"cell_type":"markdown","metadata":{"id":"c1V-2MoPZG9p"},"source":["For more information on dependencies, you can consult the following URLs:\n","* https://universaldependencies.org/u/dep/all.html\n","\n","* https://universaldependencies.org/docs/u/dep/\n","\n","* https://nlp.stanford.edu/software/dependencies_manual.pdf"]},{"cell_type":"markdown","metadata":{"id":"8Z2UhkQJASVv"},"source":["## Semantic analysis\n","\n","Semantic analysis consists of determining the meaning of the words in a text. In this session we are going to see how to determine the meaning of the different words in a text according to their grammatical category and how to obtain synonyms and antonyms, which can be very useful when it comes to facilitating the understanding of a text.\n","\n","**WordNet** is a lexical database containing nouns, adjectives, verbs and adverbs grouped into synonym sets called synsets, providing short, general definitions and storing the semantic relationships between the synonym sets. It is the most widely used lexical database for word sense disambiguation (WSD), a task that aims to assign the most appropriate concept to terms according to the context in which they appear.\n","\n","NLTK provides an interface to be able to use WordNet\n","(http://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.wordnet). In the following, we are going to see some examples to understand the potential of this lexical database.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0U5ac5ZSoYMF"},"source":["### Meaning of a term\n","\n","Given a term, WordNet allows you to find out the different meanings that word can have, as well as some examples of sentences in which it can be used.\n","\n","The function `synsets(lemma, pos=None, lang='eng')`, returns all synsets (sets of synonyms) of the specified lemma that belong to the specified POS tag. If no grammatical category is specified, it returns all synsets associated with the lemma."]},{"cell_type":"code","metadata":{"id":"VblTPOM5pNtM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9ff4f53-03e1-4aa5-e08c-551e5e40e120","executionInfo":{"status":"ok","timestamp":1740410951884,"user_tz":-60,"elapsed":5382,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"TDDQIYAwASVw"},"source":["from nltk.corpus import wordnet as wn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAEYFoWBVQZ3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2653f235-35a0-43f2-e98c-3aedb32e2832","executionInfo":{"status":"ok","timestamp":1740410959348,"user_tz":-60,"elapsed":7416,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["for syn in wn.synsets(\"car\"):\n","  print(syn, syn.definition())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Synset('car.n.01') a motor vehicle with four wheels; usually propelled by an internal combustion engine\n","Synset('car.n.02') a wheeled vehicle adapted to the rails of railroad\n","Synset('car.n.03') the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n","Synset('car.n.04') where passengers ride up and down\n","Synset('cable_car.n.01') a conveyance for passengers or freight on a cable railway\n"]}]},{"cell_type":"markdown","metadata":{"id":"1grNZuRspVxR"},"source":["In the example above we can see that the term car (‘car’) always acts as a noun (‘n’) and can have 5 different meanings (‘car.n.01’, ‘car.n.02’, ‘car.n.03’, ‘car.n.04’ and ‘cable_car.n.01’).\n","\n","\n","\n","List of POS tags used in WordNet:\n","```\n","ADJ, ADJ_SAT, ADV, NOUN, VERB = ‘a’, ‘s’, ‘r’, ‘n’, ‘v’.\n","```\n","\n","Adjectives are organised in groups containing head synsets (a) and satellite synsets (s):  \n","\n","* ADJ: adjectives have minimal meaning, e.g. ‘dry’, ‘good’ etc.\n","* ADJ_SAT: adjective that imposes additional commitments beyond the meaning of the core adjective, e.g. ‘arid’ = ‘dry’ + a particular context (could mean place or climate).\n","\n","\n","To obtain an example of a sentence in which this term is used to refer to a four-wheeled motor vehicle (‘car.n.01’) we make a call to the `examples()` function of the synset:\n"]},{"cell_type":"code","metadata":{"id":"ve1cTf0TpqpM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b8089ae-0206-4f19-8bf2-a5251e856edb","executionInfo":{"status":"ok","timestamp":1740410959366,"user_tz":-60,"elapsed":10,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["wn.synset('car.n.01').examples()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['he needs a car to get to work']"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"sKSAn222pxFM"},"source":["### Synonyms\n","\n","Although definitions help to understand the meaning of a word, sometimes it may be more useful to obtain other words with the same meaning (synonyms):"]},{"cell_type":"code","metadata":{"id":"y-nr5wi2p6Os","colab":{"base_uri":"https://localhost:8080/"},"outputId":"def7efbd-8982-45b9-9e40-572e7b8d066b","executionInfo":{"status":"ok","timestamp":1740410959384,"user_tz":-60,"elapsed":10,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["wn.synset('car.n.01').lemma_names()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['car', 'auto', 'automobile', 'machine', 'motorcar']"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["wn.synset('car.n.01').lemmas()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INFFHjQWvdYw","outputId":"eb4386f5-0c09-43bf-eb5a-af5ac1451796","executionInfo":{"status":"ok","timestamp":1740410959424,"user_tz":-60,"elapsed":39,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Lemma('car.n.01.car'),\n"," Lemma('car.n.01.auto'),\n"," Lemma('car.n.01.automobile'),\n"," Lemma('car.n.01.machine'),\n"," Lemma('car.n.01.motorcar')]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["wn.synonyms(\"car\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wj4J4ThRGONz","outputId":"16513423-e79a-4625-a7ca-19b62f06f0a6","executionInfo":{"status":"ok","timestamp":1740410959433,"user_tz":-60,"elapsed":10,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['auto', 'automobile', 'machine', 'motorcar'],\n"," ['railcar', 'railroad_car', 'railway_car'],\n"," ['gondola'],\n"," ['elevator_car'],\n"," ['cable_car']]"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"4kl-kul5p-B6"},"source":["### Antonyms\n","\n","WordNet also allows you to obtain words with an opposite meaning to a given word (antonyms).\n","\n","The following example shows how to get an antonym for the word ‘cheap’ when used with the meaning ‘low price’:\n"]},{"cell_type":"code","metadata":{"id":"pgZuHb9IqG2a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3fd3a9a-e18c-4b5f-af7f-7abcbe0e2540","executionInfo":{"status":"ok","timestamp":1740410959496,"user_tz":-60,"elapsed":59,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["for ss in wn.synsets(\"cheap\"):\n","  print(ss, ss.definition())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Synset('cheap.a.01') relatively low in price or charging low prices\n","Synset('brassy.s.02') tastelessly showy\n","Synset('bum.s.01') of very poor quality; flimsy\n","Synset('cheap.s.04') embarrassingly stingy\n"]}]},{"cell_type":"code","source":["wn.synonyms(\"cheap\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df7_wYN_sUau","outputId":"5eef6811-78ab-48a6-91ae-60c007c174ec","executionInfo":{"status":"ok","timestamp":1740410959504,"user_tz":-60,"elapsed":9,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['inexpensive'],\n"," ['brassy',\n","  'flash',\n","  'flashy',\n","  'garish',\n","  'gaudy',\n","  'gimcrack',\n","  'loud',\n","  'meretricious',\n","  'tacky',\n","  'tatty',\n","  'tawdry',\n","  'trashy'],\n"," ['bum', 'cheesy', 'chintzy', 'crummy', 'punk', 'sleazy', 'tinny'],\n"," ['chinchy', 'chintzy']]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["wn.synset(\"cheap.a.01\").lemmas()[0].antonyms()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AFEBVPyNvxqw","outputId":"9b1e7f49-2ecd-4354-8480-5ad99b1ebea1","executionInfo":{"status":"ok","timestamp":1740410959517,"user_tz":-60,"elapsed":12,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Lemma('expensive.a.01.expensive')]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"kMapk7k-rrHz"},"source":["### Disambiguation\n","\n","So far we have worked with concrete terms by selecting the synset for which we want to obtain the information, but what happens if what we have is not a term but a sentence. It will be necessary to carry out a disambiguation process taking into account the context in which the words occur.\n","\n","NLTK provides an implementation of the Lesk disambiguation algorithm (https://www.nltk.org/api/nltk.wsd.lesk.html?highlight=lesk), which, given an ambiguous word and the context in which the word occurs, returns the synset with the highest number of common words between the context of the sentence and the dictionary definition of the synset. For example, the adjective ‘cheap’ is an ambiguous word that can have one of the following meanings: relatively low in price (‘cheap.a.01’), tasteless (‘brassy.s.02’), of very poor quality; flimsy (‘bum.s.01’) or embarrassingly cheap (‘cheap.s.04’)."]},{"cell_type":"code","metadata":{"id":"QDF7H4RssHbw"},"source":["from nltk.wsd import lesk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5-jzgzdsTKc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3298a14b-34f8-4337-ba3b-02571e93ec0e","executionInfo":{"status":"ok","timestamp":1740410959547,"user_tz":-60,"elapsed":13,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["sent = 'I am surprised with the price of this restaurant , it is very cheap .'\n","sent = sent.split()\n","synset = lesk(sent, 'cheap', 'a')\n","print(synset)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Synset('cheap.a.01')\n"]}]},{"cell_type":"markdown","metadata":{"id":"DGI0myP0snsz"},"source":["If we show the definition of the synset provided by the Lesk algorithm we can see that, in that sentence, the word ‘cheap’ refers to ‘low in price’."]},{"cell_type":"code","metadata":{"id":"PAJlts_qsklI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ededcee-f19a-4fc2-b557-57c6b7a19f68","executionInfo":{"status":"ok","timestamp":1740410959583,"user_tz":-60,"elapsed":36,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"source":["print(synset.definition())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["relatively low in price or charging low prices\n"]}]},{"cell_type":"markdown","metadata":{"id":"IMKBCaDi30Tz"},"source":["## Exercises\n","\n","The exercises must be done on this *notebook*, and must be submitted through PLATEA before the deadline indicated.\n","\n","Download the story ‘carroll-alice.txt’ available in PLATEA (Supplementary Material folder) and perform the necessary methods to analyse the following information:"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YNyn9UmtWOE","executionInfo":{"status":"ok","timestamp":1740414809090,"user_tz":-60,"elapsed":1116,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"b5b4b6ec-0b4b-44f5-fe1e-a6362766c81f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/NLP/regreso_al_paraiso.txt\""],"metadata":{"id":"9wo-MvUKtmDZ","executionInfo":{"status":"ok","timestamp":1740414811416,"user_tz":-60,"elapsed":3,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8SxN0j04SAg"},"source":["### Exercise 1 - Morphological Analysis\n","\n","Obtain the total number of **nouns/nouns**, **adjectives**, **verbs** and **adverbs** in the text. For nouns, calculate also the number of:\n","* Nouns in masculine and singular.\n","* Nouns in masculine and plural.\n","* Nouns in the feminine and singular.\n","* Feminine and plural nouns.\n","\n","Are there genderless nouns?\n"]},{"cell_type":"code","source":["import spacy\n","spacy.cli.download(\"es_core_news_sm\") # Small model (sm) in English. Also available md (medium) and lg (large).\n","# nlp = spacy.load(\"en_core_web_sm\")\n","nlp = spacy.load(\"es_core_news_sm\")"],"metadata":{"id":"N51LvNIiK6L-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740414834812,"user_tz":-60,"elapsed":11892,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"4f4fc196-2928-4e1f-b1ac-697da542e5b6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["with open(data_path, 'r') as f:\n","  text = f.read()\n","\n","doc = nlp(text)"],"metadata":{"id":"qjPRUA0dud7R","executionInfo":{"status":"ok","timestamp":1740414852360,"user_tz":-60,"elapsed":16355,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["noun_count = 0\n","adj_count = 0\n","verb_count = 0\n","adv_count = 0\n","\n","masc_sing_nouns = 0\n","masc_plur_nouns = 0\n","fem_sing_nouns = 0\n","fem_plur_nouns = 0\n","genderless_nouns = 0"],"metadata":{"id":"I0XUrmufvS30","executionInfo":{"status":"ok","timestamp":1740414856447,"user_tz":-60,"elapsed":10,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["for token in doc:\n","    if token.pos_ == \"NOUN\":\n","        noun_count += 1\n","        morph = token.morph.to_dict()\n","        if morph.get(\"Number\") == \"Sing\":\n","          if morph.get(\"Gender\") == \"Masc\":\n","            masc_sing_nouns += 1\n","          elif morph.get(\"Gender\") == \"Fem\":\n","            fem_sing_nouns +=1\n","          elif morph.get(\"Gender\") == None:\n","            genderless_nouns +=1\n","        elif morph.get(\"Number\") == \"Plur\":\n","          if morph.get(\"Gender\") == \"Masc\":\n","            masc_plur_nouns += 1\n","          elif morph.get(\"Gender\") == \"Fem\":\n","            fem_plur_nouns += 1\n","          elif morph.get(\"Gender\") == None:\n","            genderless_nouns +=1\n","        elif morph.get(\"Gender\") == None and morph.get(\"Number\") == None:\n","          genderless_nouns +=1\n","\n","    elif token.pos_ == \"ADJ\":\n","        adj_count += 1\n","    elif token.pos_ == \"VERB\":\n","        verb_count += 1\n","    elif token.pos_ == \"ADV\":\n","        adv_count += 1"],"metadata":{"id":"6kT5rcWDviRL","executionInfo":{"status":"ok","timestamp":1740414858502,"user_tz":-60,"elapsed":51,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(f\"Total Nouns: {noun_count}\")\n","print(f\"Total Adjectives: {adj_count}\")\n","print(f\"Total Verbs: {verb_count}\")\n","print(f\"Total Adverbs: {adv_count}\")\n","\n","print(f\"Masculine Singular Nouns: {masc_sing_nouns}\")\n","print(f\"Masculine Plural Nouns: {masc_plur_nouns}\")\n","print(f\"Feminine Singular Nouns: {fem_sing_nouns}\")\n","print(f\"Feminine Plural Nouns: {fem_plur_nouns}\")\n","print(f\"Genderless nouns: {genderless_nouns}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTeUh_tRv2yr","executionInfo":{"status":"ok","timestamp":1740414860524,"user_tz":-60,"elapsed":12,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"ff390204-4d28-403c-d84f-c2faf2b9babc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Nouns: 6531\n","Total Adjectives: 2104\n","Total Verbs: 5280\n","Total Adverbs: 3038\n","Masculine Singular Nouns: 1723\n","Masculine Plural Nouns: 913\n","Feminine Singular Nouns: 2465\n","Feminine Plural Nouns: 917\n","Genderless nouns: 505\n"]}]},{"cell_type":"markdown","source":["as in English nouns don't have gender I have decided to do this exercise with Spanish txt file."],"metadata":{"id":"NmAxl0gPz0xc"}},{"cell_type":"markdown","source":["### Exercise 2 - Syntactic Analysis\n","\n","* What are the 5 most frequent nominal subjects?\n","* What are the 5 most frequent direct complements?\n","* What are the 5 most frequent negators?\n","* Search for information and browse the dependency tree in ***spaCy*** for the 3 most frequent words that are connected to each of the negators obtained in the previous question."],"metadata":{"id":"i7mNvaBpJOln"}},{"cell_type":"code","source":["spacy.cli.download(\"en_core_web_sm\")\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NIjngN70-zP","executionInfo":{"status":"ok","timestamp":1740415211485,"user_tz":-60,"elapsed":4389,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"c571a596-8b78-4a77-af7b-45a62660cba1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/NLP/carroll-alice.txt\"\n","with open(data_path, 'r') as f:\n","  text = f.read()\n","\n","doc = nlp(text)"],"metadata":{"id":"7hYm68ZZ1J_C","executionInfo":{"status":"ok","timestamp":1740415274636,"user_tz":-60,"elapsed":7557,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"V5B2iAJ70qYn","executionInfo":{"status":"ok","timestamp":1740415277650,"user_tz":-60,"elapsed":3,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["nominal_subjects = [token.text for token in doc if token.dep_ == \"nsubj\"]\n","subject_counts = Counter(nominal_subjects)\n","print(\"5 most frequent nominal subjects:\")\n","for subject, count in subject_counts.most_common(5):\n","    print(f\"{subject}: {count}\")"],"metadata":{"id":"U042Bd1Cvhkt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740415278749,"user_tz":-60,"elapsed":15,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"18ccbe08-daee-45f3-c73f-10c09326cd19"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["5 most frequent nominal subjects:\n","I: 516\n","she: 502\n","Alice: 286\n","it: 281\n","you: 280\n"]}]},{"cell_type":"code","source":["direct_complements = [token.text for token in doc if token.dep_ == \"dobj\"]\n","complement_counts = Counter(direct_complements)\n","print(\"\\n5 most frequent direct complements:\")\n","for complement, count in complement_counts.most_common(5):\n","    print(f\"{complement}: {count}\")"],"metadata":{"id":"3kVaa-FUK5I3","executionInfo":{"status":"ok","timestamp":1740415282432,"user_tz":-60,"elapsed":19,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"431b2a15-8dc3-4454-804d-7c8ca07663fc"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","5 most frequent direct complements:\n","it: 125\n","what: 40\n","them: 36\n","Alice: 30\n","her: 28\n"]}]},{"cell_type":"code","source":["negators = [token.text for token in doc if token.dep_ == \"neg\"]\n","negator_counts = Counter(negators)\n","print(\"\\n5 most frequent negators:\")\n","for negator, count in negator_counts.most_common(5):\n","    print(f\"{negator}: {count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aryZA9Ei014x","executionInfo":{"status":"ok","timestamp":1740415289566,"user_tz":-60,"elapsed":20,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"d0eb7a3b-76e2-402b-83f2-3e8cd8b7b0f3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","5 most frequent negators:\n","n't: 215\n","not: 125\n","never: 39\n","Not: 9\n","no: 6\n"]}]},{"cell_type":"code","source":["print(\"\\nWords connected to the top 3 negators:\")\n","for negator, count in negator_counts.most_common(3):\n","    print(f\"\\nNegator: '{negator}' (occurs {count} times)\")\n","    connected_words = []  # List to store words connected to the negator\n","\n","    for token in doc:\n","        if token.dep_ == \"neg\" and token.text == negator:\n","            # Find the head word, which is the word the negator modifies\n","            head_word = token.head.text\n","            connected_words.append(head_word)  # Add the head word to the list\n","\n","            # Find child words that depend on the head word (excluding the negator itself)\n","            for child in token.head.children:\n","                if child.dep_ != 'neg':\n","                    connected_words.append(child.text)\n","\n","    # Count the frequency of connected words\n","    connected_counts = Counter(connected_words)\n","\n","    # Print the 3 most frequent connected words\n","    for word, count in connected_counts.most_common(3):\n","        print(f\"  Word '{word}' (occurs {count} times)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pjc1mRzV05vs","executionInfo":{"status":"ok","timestamp":1740415541289,"user_tz":-60,"elapsed":45,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"8bba3a25-c132-4929-edfa-71c9682df05e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Words connected to the top 3 negators:\n","\n","Negator: 'n't' (occurs 215 times)\n","  Word ''' (occurs 105 times)\n","  Word 'I' (occurs 81 times)\n","  Word ',' (occurs 76 times)\n","\n","Negator: 'not' (occurs 125 times)\n","  Word ',' (occurs 81 times)\n","  Word 'could' (occurs 40 times)\n","  Word ''' (occurs 28 times)\n","\n","Negator: 'never' (occurs 39 times)\n","  Word ',' (occurs 20 times)\n","  Word 'I' (occurs 14 times)\n","  Word ''' (occurs 14 times)\n"]}]},{"cell_type":"code","source":["print(\"\\nWords connected to the top 3 negators:\")\n","for negator, count in negator_counts.most_common(3):\n","    print(f\"\\nNegator: '{negator}' (occurs {count} times)\")\n","    connected_words = []  # List to store words connected to the negator\n","\n","    for token in doc:\n","        if token.dep_ == \"neg\" and token.text == negator:\n","            # Find the head word, which is the word the negator modifies\n","            head_word = token.head.text\n","            connected_words.append(head_word)  # Add the head word to the list\n","\n","            # Find child words that depend on the head word (excluding the negator itself)\n","            for child in token.head.children:\n","                if child.dep_ != 'neg' and child.is_alpha:\n","                    connected_words.append(child.text)\n","\n","    # Count the frequency of connected words\n","    connected_counts = Counter(connected_words)\n","\n","    # Print the 3 most frequent connected words\n","    for word, count in connected_counts.most_common(3):\n","        print(f\"  Word '{word}' (occurs {count} times)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlU1Ffop2rCL","executionInfo":{"status":"ok","timestamp":1740415673034,"user_tz":-60,"elapsed":23,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"22d31b2c-e320-41fa-f885-6dc12b788cf9"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Words connected to the top 3 negators:\n","\n","Negator: 'n't' (occurs 215 times)\n","  Word 'I' (occurs 81 times)\n","  Word 'do' (occurs 54 times)\n","  Word 'you' (occurs 46 times)\n","\n","Negator: 'not' (occurs 125 times)\n","  Word 'could' (occurs 40 times)\n","  Word 'did' (occurs 24 times)\n","  Word 'she' (occurs 21 times)\n","\n","Negator: 'never' (occurs 39 times)\n","  Word 'I' (occurs 14 times)\n","  Word 'had' (occurs 11 times)\n","  Word 'before' (occurs 6 times)\n"]}]},{"cell_type":"markdown","source":["### Exercise 3 - Semantic analysis\n","For each of the semantic analysis exercises you will use the document ‘carroll-alice.txt’, available in PLATEA.\n","\n","1. Find a synonym for each of the adjectives in the text and generate a .json file containing the frequency of each substitution made:\n","\n","\n","```\n","{\n","    (adj_original_1, synonym): pair_frequency\n","    (adj_original_2, synonym): even_frequency\n","    ...\n","}\n","```\n","\n","2. Obtain the possible **meanings**, **synonyms** and **antonyms** of the most frequent adjective in the text.\n","3. Generate a new file from the disambiguation of all the nouns in the document. To do this, the nouns must be replaced by the tuple (word, synset).\n","4. Obtain the most frequent sense without considering empty words. Note that they are relevant for disambiguation."],"metadata":{"id":"B9xVZzFPK6oJ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0rCfhPS3H_-","executionInfo":{"status":"ok","timestamp":1740415756340,"user_tz":-60,"elapsed":2512,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"950a14b7-85f5-4e18-fb93-00cce11f21e6"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["from nltk.corpus import wordnet as wn\n","import json"],"metadata":{"id":"RvXavcx73N0n","executionInfo":{"status":"ok","timestamp":1740415814619,"user_tz":-60,"elapsed":37,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["**1st task**\n","\n","Find a synonym for each of the adjectives in the text and generate a .json file containing the frequency of each substitution made"],"metadata":{"id":"Wg8o-0QD5rnm"}},{"cell_type":"code","source":["substitution_counts = Counter()\n","\n","for token in doc:\n","    if token.pos_ == \"ADJ\":\n","        adj_original = token.text\n","        synonyms = []\n","        for syn in wn.synsets(adj_original, pos=wn.ADJ):\n","            for lemma in syn.lemmas():\n","                synonyms.append(lemma.name())\n","        if synonyms:\n","            synonym = synonyms[0]\n","            substitution_counts[(adj_original, synonym)] += 1"],"metadata":{"id":"08UyI0cy3QDA","executionInfo":{"status":"ok","timestamp":1740416380689,"user_tz":-60,"elapsed":28,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["output_file = \"adjective_synonym_substitutions.json\"\n","substitution_counts_str = {f\"{adj}, {syn}\": count for (adj, syn), count in substitution_counts.items()}\n"],"metadata":{"id":"LlN2M2Um4CwJ","executionInfo":{"status":"ok","timestamp":1740416382930,"user_tz":-60,"elapsed":14,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["with open(output_file, \"w\") as f:\n","    json.dump(substitution_counts_str, f, indent=4)"],"metadata":{"id":"ZPiwu3kZ4Hpt","executionInfo":{"status":"ok","timestamp":1740416383875,"user_tz":-60,"elapsed":9,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["**2nd task**\n","\n","Obtain the possible meanings, synonyms and antonyms of the most frequent adjective in the text."],"metadata":{"id":"EuoAISXE5yO_"}},{"cell_type":"code","source":["adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n","adj_counts = Counter(adjectives)"],"metadata":{"id":"KvH3FP1h4fll","executionInfo":{"status":"ok","timestamp":1740416476439,"user_tz":-60,"elapsed":26,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["most_frequent_adj = adj_counts.most_common(1)[0][0]\n","print(f\"Most frequent adjective: {most_frequent_adj}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GrVne5B35-Ql","executionInfo":{"status":"ok","timestamp":1740416520911,"user_tz":-60,"elapsed":15,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"5ecabb22-1a7b-4a58-caa5-1fbc9751ed35"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Most frequent adjective: little\n"]}]},{"cell_type":"code","source":["synsets = wn.synsets(most_frequent_adj, pos=wn.ADJ)"],"metadata":{"id":"I6NQgJpG6Z1r","executionInfo":{"status":"ok","timestamp":1740416604996,"user_tz":-60,"elapsed":29,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["if synsets:\n","    print(\"\\nPossible meanings:\")\n","    for synset in synsets:\n","        print(f\"- {synset.definition()}\")\n","\n","    print(\"\\nSynonyms:\")\n","    for synset in synsets:\n","        for lemma in synset.lemmas():\n","            if lemma.name() != most_frequent_adj:  # Avoid duplicating the original word\n","                print(f\"- {lemma.name()}\")\n","\n","    print(\"\\nAntonyms:\")\n","    for synset in synsets:\n","        for lemma in synset.lemmas():\n","            for antonym in lemma.antonyms():\n","                print(f\"- {antonym.name()}\")\n","else:\n","    print(\"Synonyms and antonyms not found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvUG63i76AUt","executionInfo":{"status":"ok","timestamp":1740416607505,"user_tz":-60,"elapsed":22,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"5c6a8180-b786-4b6e-8b72-c521f443123e"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Possible meanings:\n","- limited or below average in number or quantity or magnitude or extent\n","- (quantifier used with mass nouns) small in quantity or degree; not much or almost none or (with `a') at least some\n","- (of children and animals) young, immature\n","- (informal) small and of little importance\n","- (of a voice) faint\n","- low in stature; not tall\n","- lowercase\n","- small in a way that arouses feelings (of tenderness or its opposite depending on the context)\n","\n","Synonyms:\n","- small\n","- slight\n","- small\n","- fiddling\n","- footling\n","- lilliputian\n","- niggling\n","- piddling\n","- piffling\n","- petty\n","- picayune\n","- trivial\n","- small\n","- short\n","- minuscule\n","- small\n","\n","Antonyms:\n","- large\n","- big\n","- much\n","- tall\n"]}]},{"cell_type":"markdown","source":["**3rd task**\n","\n","Generate a new file from the disambiguation of all the nouns in the document. To do this, the nouns must be replaced by the tuple (word, synset)."],"metadata":{"id":"YnM1IG216j1Q"}},{"cell_type":"code","source":["modified_tokens = []\n","for token in doc:\n","    if token.pos_ == \"NOUN\":\n","        # Attempt to disambiguate the noun using WordNet\n","        synsets = wn.synsets(token.text, pos=wn.NOUN)\n","        if synsets:\n","            synset = synsets[0]\n","            modified_tokens.append(f\"({token.text}, {synset.name()})\")\n","        else:\n","            # If no synset is found, keep the original noun\n","            modified_tokens.append(token.text)\n","    else:\n","        # Keep non-noun tokens as they are\n","        modified_tokens.append(token.text)"],"metadata":{"id":"KtdqAZff6UDP","executionInfo":{"status":"ok","timestamp":1740416860380,"user_tz":-60,"elapsed":101,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["new_text = \" \".join(modified_tokens)\n","\n","output_file = \"carroll-alice_nouns_disambiguated.txt\"\n","with open(output_file, \"w\", encoding=\"utf-8\") as f:\n","    f.write(new_text)"],"metadata":{"id":"_08sVUnp7U7b","executionInfo":{"status":"ok","timestamp":1740416848006,"user_tz":-60,"elapsed":34,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["**4th task**\n","\n","Obtain the most frequent sense without considering empty words. Note that they are relevant for disambiguation."],"metadata":{"id":"TDulpnvp6i6N"}},{"cell_type":"code","source":["from nltk.corpus import  stopwords\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_7vz2Qw8ghd","executionInfo":{"status":"ok","timestamp":1740417205261,"user_tz":-60,"elapsed":55,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"2e5af1f1-de6f-450b-d52b-d4eeaa9d74d7"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))"],"metadata":{"id":"ENpWQFGs89Bk","executionInfo":{"status":"ok","timestamp":1740417260180,"user_tz":-60,"elapsed":13,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["synset_counts = Counter()\n","for token in doc:\n","    if token.pos_ == \"NOUN\":\n","        synsets = wn.synsets(token.text, pos=wn.NOUN)\n","        if synsets:\n","            synset = synsets[0]\n","            if token.text.lower() not in stop_words:\n","              synset_counts[synset] += 1"],"metadata":{"id":"QFCvZ5iy8wMC","executionInfo":{"status":"ok","timestamp":1740417325182,"user_tz":-60,"elapsed":44,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["most_frequent_synset = synset_counts.most_common(1)\n","\n","if most_frequent_synset:\n","    most_frequent_synset = most_frequent_synset[0][0]\n","    print(f\"Most frequent synset (excluding stop words): {most_frequent_synset.name()}\")\n","    print(f\"Definition: {most_frequent_synset.definition()}\")\n","else:\n","    print(\"No nouns found (excluding stop words) with synsets.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjEdIUJK9Gzc","executionInfo":{"status":"ok","timestamp":1740417331677,"user_tz":-60,"elapsed":20,"user":{"displayName":"Yuliya Dabreha","userId":"05622474096039686923"}},"outputId":"88760b7c-c91a-4dd1-d8a8-6f8a96499114"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Most frequent synset (excluding stop words): time.n.01\n","Definition: an instance or single occasion for some event\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"g8vrq5FL9PDy"},"execution_count":null,"outputs":[]}]}